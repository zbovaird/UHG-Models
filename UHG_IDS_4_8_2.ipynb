{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNdDsyTs9jIhHo9Ti/+nNdC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zbovaird/UHG-Models/blob/main/UHG_IDS_4_8_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcDzYr2EmTNh",
        "outputId": "fb2763d9-9532-40db-d140-a336b8ef01ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.9/1.8 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install PyTorch Geometric (matches your current torch/cuda)\n",
        "!pip -q install --upgrade pip\n",
        "import torch\n",
        "pt = torch.__version__.split('+')[0]\n",
        "cuda = torch.version.cuda\n",
        "if torch.cuda.is_available() and cuda:\n",
        "  idx = f\"https://data.pyg.org/whl/torch-{pt}+cu{cuda.replace('.','')}.html\"\n",
        "else:\n",
        "  idx = f\"https://data.pyg.org/whl/torch-{pt}+cpu.html\"\n",
        "\n",
        "!pip -q install torch_scatter torch_sparse torch_cluster torch_spline_conv -f {idx}\n",
        "!pip -q install torch_geometric scikit-learn scipy pandas tqdm\n",
        "\n",
        "# Install PyNNDescent for ultra-fast approximate KNN\n",
        "!pip -q install pynndescent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Intrusion Detection using Universal Hyperbolic Geometry (UHG) v4.8.2\n",
        "üî¨ PyNNDescent + DAMPENED Class Weights @ 20% Data (PRECISION FIX!)\n",
        "\n",
        "v4.8.2 CONFIGURATION:\n",
        "- PyNNDescent for approximate KNN (PROVEN: 3x faster, <2% accuracy loss!) ‚úÖ\n",
        "- 20% data sampling (566k samples) - Perfect fit for L4 GPU! ‚úÖ\n",
        "- DAMPENED CLASS WEIGHTS - Fixes precision collapse! ‚≠ê NEW!\n",
        "- Expected KNN time: ~10-15s üöÄ\n",
        "- Expected total runtime: ~2-3 minutes ‚ö°\n",
        "\n",
        "Problem Identified from v4.8.1:\n",
        "- Inverse frequency weights cause EXTREME values (Bot: 97.28, Heartbleed: 9435.8)\n",
        "- Result: 100% recall but <2% precision (98% false positives!)\n",
        "- BENIGN recall drops to 83% (17% benign traffic misclassified!)\n",
        "\n",
        "v4.8.2 Solution: Dampened Class Weights\n",
        "We test 3 strategies to balance recall vs precision:\n",
        "\n",
        "OPTION 1: Square Root Dampening\n",
        "  weight = sqrt(total / (num_classes * count))\n",
        "  Effect: Reduces extreme weights (9435.8 ‚Üí 97.1)\n",
        "  Best for: Moderate imbalance (10-100x difference)\n",
        "\n",
        "OPTION 2: Log Dampening\n",
        "  weight = log(1 + total / (num_classes * count))\n",
        "  Effect: Strongest dampening (9435.8 ‚Üí 9.15)\n",
        "  Best for: Extreme imbalance (>1000x difference)\n",
        "\n",
        "OPTION 3: Capped Weights\n",
        "  weight = clip(total / (num_classes * count), min=0.1, max=100)\n",
        "  Effect: Hard limits on weights\n",
        "  Best for: Preventing outliers\n",
        "\n",
        "Expected Results @ 20%:\n",
        "OPTION 1 (sqrt): Best balance - 93-95% accuracy, 50-70% precision, 95%+ recall\n",
        "OPTION 2 (log):  More conservative - 95-96% accuracy, 70-90% precision, 85-95% recall\n",
        "OPTION 3 (cap):  Middle ground - 94-95% accuracy, 60-80% precision, 90-95% recall\n",
        "\n",
        "v4.8.2 Features:\n",
        "- DAMPENED CLASS WEIGHTS for precision/recall balance ‚≠ê NEW!\n",
        "- PyNNDescent for ultra-fast approximate KNN (proven!)\n",
        "- PCA dimensionality reduction (77‚Üí20 dims, 89% variance)\n",
        "- k=2 neighbors (optimized for speed)\n",
        "- Fixed evaluation for missing classes in test set\n",
        "- Comprehensive timing instrumentation and bottleneck analysis\n",
        "- GPU detection and memory usage tracking\n",
        "- UHG constraint verification post-training\n",
        "\n",
        "Compare to Previous Versions:\n",
        "v4.5 @ 10% (inverse weights): 87.20% acc, Bot 1% precision ‚ùå\n",
        "v4.8.1 @ 10% (inverse weights): 86.00% acc, Bot 2% precision ‚ùå\n",
        "v4.8 @ 20% (NO weights):       96.90% acc, 0% minority recall ‚ùå\n",
        "v4.8.2 @ 20% (DAMPENED):       ???% acc, ???% precision, ???% recall üî¨\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.sparse import coo_matrix\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torch_geometric.data import Data\n",
        "from typing import Tuple\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "import traceback\n",
        "import platform\n",
        "from datetime import datetime\n",
        "\n",
        "# Import PyNNDescent\n",
        "from pynndescent import NNDescent\n",
        "\n",
        "# Optional: Drive mount (only in Colab)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    print(\"Mounting Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# ======================\n",
        "# CONFIGURATION\n",
        "# ======================\n",
        "# ‚ö†Ô∏è CHANGE THIS TO TEST DIFFERENT STRATEGIES! ‚ö†Ô∏è\n",
        "WEIGHTING_STRATEGY = \"sqrt\"  # Options: \"sqrt\", \"log\", \"capped\"\n",
        "# ======================\n",
        "\n",
        "# Device configuration with detailed GPU info\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üñ•Ô∏è  HARDWARE CONFIGURATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB\n",
        "    cuda_version = torch.version.cuda\n",
        "    gpu_capability = torch.cuda.get_device_capability(0)\n",
        "\n",
        "    print(f\"‚úÖ GPU Detected:\")\n",
        "    print(f\"   ‚Ä¢ Model: {gpu_name}\")\n",
        "    print(f\"   ‚Ä¢ Memory: {gpu_memory:.1f} GB\")\n",
        "    print(f\"   ‚Ä¢ CUDA Version: {cuda_version}\")\n",
        "    print(f\"   ‚Ä¢ Compute Capability: {gpu_capability[0]}.{gpu_capability[1]}\")\n",
        "    print(f\"   ‚Ä¢ Device: cuda:0\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  No GPU available - using CPU\")\n",
        "    print(f\"   ‚Ä¢ This will be significantly slower for training\")\n",
        "\n",
        "print(f\"\\nüî¨ Configuration (v4.8.2 - Dampened Class Weights):\")\n",
        "print(f\"   ‚Ä¢ KNN Method: PyNNDescent (Approximate) ‚≠ê PROVEN!\")\n",
        "print(f\"   ‚Ä¢ Loss: DAMPENED CLASS-WEIGHTED CrossEntropyLoss ‚öñÔ∏è\")\n",
        "print(f\"   ‚Ä¢ Weighting Strategy: {WEIGHTING_STRATEGY.upper()} üî¨\")\n",
        "print(f\"   ‚Ä¢ Data: 20% sampling (566k samples) - Max for L4 GPU!\")\n",
        "print(f\"   ‚Ä¢ Expected KNN time: ~10-15s (vs ~70s sklearn!)\")\n",
        "print(f\"   ‚Ä¢ Expected total time: ~2-3 minutes\")\n",
        "print(f\"   ‚Ä¢ Goal: Fix precision collapse (1-2% ‚Üí 50-90%!)\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "if WEIGHTING_STRATEGY == \"sqrt\":\n",
        "    print(\"üìä SQRT DAMPENING:\")\n",
        "    print(\"   ‚Ä¢ Formula: weight = sqrt(total / (num_classes * count))\")\n",
        "    print(\"   ‚Ä¢ Effect: Moderate dampening (9435 ‚Üí 97)\")\n",
        "    print(\"   ‚Ä¢ Best for: 10-100x class imbalance\")\n",
        "    print(\"   ‚Ä¢ Expected: 93-95% accuracy, 50-70% precision, 95%+ recall\")\n",
        "elif WEIGHTING_STRATEGY == \"log\":\n",
        "    print(\"üìä LOG DAMPENING:\")\n",
        "    print(\"   ‚Ä¢ Formula: weight = log(1 + total / (num_classes * count))\")\n",
        "    print(\"   ‚Ä¢ Effect: Strong dampening (9435 ‚Üí 9.2)\")\n",
        "    print(\"   ‚Ä¢ Best for: >1000x class imbalance\")\n",
        "    print(\"   ‚Ä¢ Expected: 95-96% accuracy, 70-90% precision, 85-95% recall\")\n",
        "elif WEIGHTING_STRATEGY == \"capped\":\n",
        "    print(\"üìä CAPPED WEIGHTS:\")\n",
        "    print(\"   ‚Ä¢ Formula: weight = clip(total / (num_classes * count), 0.1, 100)\")\n",
        "    print(\"   ‚Ä¢ Effect: Hard limits on extremes\")\n",
        "    print(\"   ‚Ä¢ Best for: Preventing outliers\")\n",
        "    print(\"   ‚Ä¢ Expected: 94-95% accuracy, 60-80% precision, 90-95% recall\")\n",
        "print()\n",
        "\n",
        "# ======================\n",
        "# File Configuration\n",
        "# ======================\n",
        "FILE_PATH = '/content/drive/MyDrive/CIC_data.csv'\n",
        "MODEL_SAVE_PATH = '/content/drive/MyDrive/uhg_ids_model_best.pth'\n",
        "RESULTS_PATH = '/content/drive/MyDrive/uhg_ids_results/'\n",
        "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
        "\n",
        "def get_env_info():\n",
        "    return {\n",
        "        'python': platform.python_version(),\n",
        "        'torch': torch.__version__,\n",
        "        'cuda_available': torch.cuda.is_available(),\n",
        "        'device': str(device),\n",
        "    }\n",
        "\n",
        "# ===================\n",
        "# UHG Geometry Helpers\n",
        "# ===================\n",
        "\n",
        "def minkowski_inner_product(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Minkowski inner product: ‚ü®x,y‚ü©_M = ‚àëx_i*y_i - x_t*y_t\"\"\"\n",
        "    spatial = (x[..., :-1] * y[..., :-1]).sum(dim=-1)\n",
        "    time = x[..., -1] * y[..., -1]\n",
        "    return spatial - time\n",
        "\n",
        "def projective_normalize(x: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
        "    \"\"\"Normalize projective coordinates: x¬≤ + y¬≤ - z¬≤ = -1\"\"\"\n",
        "    spatial_norm_sq = (x[..., :-1] ** 2).sum(dim=-1, keepdim=True)\n",
        "    z = torch.sqrt(torch.clamp(spatial_norm_sq + 1.0, min=eps))\n",
        "    return torch.cat([x[..., :-1], z], dim=-1)\n",
        "\n",
        "def uhg_quadrance_vectorized(x: torch.Tensor, y: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
        "    \"\"\"Compute vectorized UHG quadrance between vectors\"\"\"\n",
        "    numerator = minkowski_inner_product(x, y)\n",
        "    denom_x = torch.clamp(-minkowski_inner_product(x, x), min=eps)\n",
        "    denom_y = torch.clamp(-minkowski_inner_product(y, y), min=eps)\n",
        "    cos_val = numerator / torch.sqrt(denom_x * denom_y)\n",
        "    cos_val = torch.clamp(cos_val, min=-1.0+eps, max=1.0-eps)\n",
        "    return 1 - cos_val**2\n",
        "\n",
        "def verify_uhg_constraints(x: torch.Tensor, name: str = \"embeddings\"):\n",
        "    \"\"\"Verify Minkowski norm constraints\"\"\"\n",
        "    norm_sq = minkowski_inner_product(x, x)\n",
        "    violation = torch.abs(norm_sq + 1.0)\n",
        "    max_viol = violation.max().item()\n",
        "    mean_viol = violation.mean().item()\n",
        "    print(f\"UHG Constraint Check ({name}):\")\n",
        "    print(f\"  Max violation: {max_viol:.6f}\")\n",
        "    print(f\"  Mean violation: {mean_viol:.6f}\")\n",
        "    if max_viol > 0.01:\n",
        "        print(f\"  ‚ö†Ô∏è WARNING: Constraints violated!\")\n",
        "    else:\n",
        "        print(f\"  ‚úÖ Constraints satisfied\")\n",
        "\n",
        "# ====================\n",
        "# Data Loading\n",
        "# ====================\n",
        "\n",
        "def compute_class_weights(class_counts: np.ndarray, strategy: str = \"sqrt\") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute class weights with different dampening strategies\n",
        "\n",
        "    Args:\n",
        "        class_counts: Array of sample counts per class\n",
        "        strategy: \"sqrt\", \"log\", or \"capped\"\n",
        "\n",
        "    Returns:\n",
        "        Array of class weights\n",
        "    \"\"\"\n",
        "    total_samples = class_counts.sum()\n",
        "    num_classes = len(class_counts)\n",
        "\n",
        "    # Compute raw inverse frequency weights\n",
        "    raw_weights = total_samples / (num_classes * class_counts)\n",
        "\n",
        "    if strategy == \"sqrt\":\n",
        "        # Square root dampening\n",
        "        weights = np.sqrt(raw_weights)\n",
        "        print(f\"   ‚Ä¢ Strategy: SQRT dampening\")\n",
        "        print(f\"   ‚Ä¢ Formula: sqrt(total / (num_classes * count))\")\n",
        "    elif strategy == \"log\":\n",
        "        # Logarithmic dampening\n",
        "        weights = np.log1p(raw_weights)\n",
        "        print(f\"   ‚Ä¢ Strategy: LOG dampening\")\n",
        "        print(f\"   ‚Ä¢ Formula: log(1 + total / (num_classes * count))\")\n",
        "    elif strategy == \"capped\":\n",
        "        # Hard cap on weights\n",
        "        weights = np.clip(raw_weights, 0.1, 100.0)\n",
        "        print(f\"   ‚Ä¢ Strategy: CAPPED weights\")\n",
        "        print(f\"   ‚Ä¢ Formula: clip(total / (num_classes * count), 0.1, 100)\")\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown weighting strategy: {strategy}\")\n",
        "\n",
        "    return weights\n",
        "\n",
        "def load_and_preprocess_data(file_path: str = FILE_PATH, sample_frac: float = 0.20, weighting_strategy: str = \"sqrt\") -> Tuple[torch.Tensor, torch.Tensor, dict, torch.Tensor, dict]:\n",
        "    \"\"\"\n",
        "    v4.8.2: Load and preprocess data (WITH dampened class weighting, 20% sampling for L4 GPU)\n",
        "\n",
        "    Returns:\n",
        "        node_features: torch.Tensor of shape (n_samples, n_features)\n",
        "        labels_tensor: torch.Tensor of shape (n_samples,)\n",
        "        label_mapping: dict mapping label names to indices\n",
        "        class_weights: torch.Tensor of shape (num_classes,) - dampened weights\n",
        "        timings: dict of timing information\n",
        "    \"\"\"\n",
        "    timings = {}\n",
        "\n",
        "    print(f\"\\nLoading data from: {file_path}\")\n",
        "    t0 = time.perf_counter()\n",
        "    data = pd.read_csv(file_path, low_memory=False)\n",
        "    timings['csv_read'] = time.perf_counter() - t0\n",
        "    print(f\"  ‚è±Ô∏è  CSV read: {timings['csv_read']:.2f}s\")\n",
        "\n",
        "    # Strip whitespace from column names and label values (matching v4.6)\n",
        "    t0 = time.perf_counter()\n",
        "    data.columns = data.columns.str.strip()\n",
        "    data['Label'] = data['Label'].str.strip()\n",
        "    timings['column_cleanup'] = time.perf_counter() - t0\n",
        "\n",
        "    unique_labels = data['Label'].unique()\n",
        "    print(f\"\\nUnique labels in the dataset: {unique_labels}\")\n",
        "    label_counts = data['Label'].value_counts()\n",
        "    print(\"\\nLabel distribution in the dataset:\")\n",
        "    print(label_counts)\n",
        "\n",
        "    # Simple random sampling\n",
        "    print(f\"\\nApplying random sampling (frac={sample_frac})...\")\n",
        "    t0 = time.perf_counter()\n",
        "    data_sampled = data.sample(frac=sample_frac, random_state=42)\n",
        "    timings['sampling'] = time.perf_counter() - t0\n",
        "    print(f\"  ‚è±Ô∏è  Sampling: {timings['sampling']:.2f}s\")\n",
        "\n",
        "    print(f\"\\nSampled label distribution:\")\n",
        "    sampled_label_counts = data_sampled['Label'].value_counts()\n",
        "    print(sampled_label_counts)\n",
        "\n",
        "    # Convert to numeric and handle missing values\n",
        "    t0 = time.perf_counter()\n",
        "    data_numeric = data_sampled.apply(pd.to_numeric, errors='coerce')\n",
        "    timings['to_numeric'] = time.perf_counter() - t0\n",
        "    print(f\"  ‚è±Ô∏è  Convert to numeric: {timings['to_numeric']:.2f}s\")\n",
        "\n",
        "    # Fill NaN and inf\n",
        "    t0 = time.perf_counter()\n",
        "    data_filled = data_numeric.fillna(data_numeric.mean())\n",
        "    data_filled = data_filled.replace([np.inf, -np.inf], np.nan)\n",
        "    data_filled = data_filled.fillna(data_filled.max())\n",
        "    if data_filled.isnull().values.any():\n",
        "        data_filled = data_filled.fillna(0)\n",
        "    timings['fillna'] = time.perf_counter() - t0\n",
        "    print(f\"  ‚è±Ô∏è  Fill NaN/inf: {timings['fillna']:.2f}s\")\n",
        "\n",
        "    labels = data_sampled['Label']\n",
        "    features = data_filled.drop(columns=['Label'])\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    scaler = StandardScaler()\n",
        "    features_scaled = scaler.fit_transform(features)\n",
        "    timings['scaling'] = time.perf_counter() - t0\n",
        "    print(f\"  ‚è±Ô∏è  Scaling: {timings['scaling']:.2f}s\")\n",
        "\n",
        "    unique_labels = sorted(labels.unique())\n",
        "    label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "    labels_numeric = labels.map(label_mapping).values\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    node_features = torch.tensor(features_scaled, dtype=torch.float32)\n",
        "    labels_tensor = torch.tensor(labels_numeric, dtype=torch.long)\n",
        "    timings['to_tensors'] = time.perf_counter() - t0\n",
        "    print(f\"  ‚è±Ô∏è  Convert to tensors: {timings['to_tensors']:.2f}s\")\n",
        "\n",
        "    print(\"\\nPreprocessing complete.\")\n",
        "    print(f\"Feature shape: {node_features.shape}\")\n",
        "    print(f\"Number of unique labels: {len(unique_labels)}\")\n",
        "\n",
        "    # Show class distribution for reference\n",
        "    class_counts = np.bincount(labels_numeric)\n",
        "    print(\"\\nClass distribution in processed data:\")\n",
        "    for label, idx in sorted(label_mapping.items(), key=lambda x: x[1]):\n",
        "        count = class_counts[idx]\n",
        "        pct = (count / len(labels_numeric)) * 100\n",
        "        print(f\"  {label:30s}: {count:7d} samples ({pct:5.2f}%)\")\n",
        "\n",
        "    # Compute class weights with dampening\n",
        "    print(f\"\\n‚öñÔ∏è  Computing Dampened Class Weights ({weighting_strategy.upper()}):\")\n",
        "    class_weights = compute_class_weights(class_counts, strategy=weighting_strategy)\n",
        "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
        "\n",
        "    # Compare with raw inverse frequency weights\n",
        "    total_samples = len(labels_numeric)\n",
        "    num_classes = len(label_mapping)\n",
        "    raw_weights = total_samples / (num_classes * class_counts)\n",
        "\n",
        "    print(f\"\\n   Weights comparison (Dampened vs Raw Inverse):\")\n",
        "    print(f\"   {'Class':<30s} {'Dampened':>10s} {'Raw':>10s} {'Reduction':>10s}\")\n",
        "    print(f\"   {'-'*65}\")\n",
        "    for label, idx in sorted(label_mapping.items(), key=lambda x: x[1]):\n",
        "        dampened = class_weights[idx]\n",
        "        raw = raw_weights[idx]\n",
        "        reduction = (1 - dampened/raw) * 100 if raw > 0 else 0\n",
        "        print(f\"   {label:30s} {dampened:10.4f} {raw:10.1f} {reduction:9.1f}%\")\n",
        "\n",
        "    timings['total'] = sum(timings.values())\n",
        "    print(f\"\\n‚è±Ô∏è  Total data loading time: {timings['total']:.2f}s\")\n",
        "\n",
        "    return node_features, labels_tensor, label_mapping, class_weights_tensor, timings\n",
        "\n",
        "# =========================\n",
        "# Graph construction (KNN)\n",
        "# =========================\n",
        "\n",
        "def create_graph_data(node_features: torch.Tensor, labels: torch.Tensor, k: int = 2, use_pca: bool = True, pca_components: int = 20) -> Tuple[Data, dict]:\n",
        "    \"\"\"v4.8.2: PyNNDescent approximate KNN with PCA dimensionality reduction\"\"\"\n",
        "    timings = {}\n",
        "\n",
        "    print(\"\\nCreating graph structure...\")\n",
        "    t0 = time.perf_counter()\n",
        "    features_np = node_features.cpu().numpy()\n",
        "    timings['to_numpy'] = time.perf_counter() - t0\n",
        "\n",
        "    # PCA for faster KNN\n",
        "    if use_pca and features_np.shape[1] > pca_components:\n",
        "        print(f\"\\nApplying PCA for faster KNN...\")\n",
        "        print(f\"  ‚Ä¢ Original features: {features_np.shape[1]}\")\n",
        "        t0 = time.perf_counter()\n",
        "        pca = PCA(n_components=pca_components)\n",
        "        features_reduced = pca.fit_transform(features_np)\n",
        "        timings['pca'] = time.perf_counter() - t0\n",
        "        explained_var = pca.explained_variance_ratio_.sum()\n",
        "        print(f\"  ‚Ä¢ Reduced features: {features_reduced.shape[1]}\")\n",
        "        print(f\"  ‚Ä¢ Explained variance: {explained_var:.4f} ({explained_var*100:.2f}%)\")\n",
        "        print(f\"  ‚è±Ô∏è  PCA: {timings['pca']:.2f}s\")\n",
        "        features_for_knn = features_reduced\n",
        "    else:\n",
        "        features_for_knn = features_np\n",
        "        timings['pca'] = 0.0\n",
        "\n",
        "    # ===== v4.8.2: PyNNDescent Approximate KNN =====\n",
        "    print(f\"\\nüöÄ Computing KNN graph with PyNNDescent (k={k})...\")\n",
        "    print(f\"  ‚Ä¢ Input shape: {features_for_knn.shape}\")\n",
        "    print(f\"  ‚Ä¢ Number of samples: {features_for_knn.shape[0]:,}\")\n",
        "    print(f\"  ‚Ä¢ Number of features: {features_for_knn.shape[1]}\")\n",
        "    print(f\"  ‚Ä¢ Using PyNNDescent (approximate)\")\n",
        "    print(f\"  ‚Ä¢ Algorithm: Nearest Neighbor Descent\")\n",
        "    print(f\"  ‚Ä¢ Expected: 10-20x faster than sklearn!\")\n",
        "    print(f\"  ‚Ä¢ Accuracy: 95-98% of exact KNN\")\n",
        "\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "\n",
        "    # Build PyNNDescent index\n",
        "    print(f\"  ‚Ä¢ Building NNDescent index...\")\n",
        "    index = NNDescent(\n",
        "        features_for_knn,\n",
        "        n_neighbors=k+1,  # +1 to include self, will remove later\n",
        "        metric='euclidean',\n",
        "        n_jobs=-1,  # Use all CPU cores\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    # Get neighbor indices (includes self as first neighbor)\n",
        "    indices, distances = index.neighbor_graph\n",
        "\n",
        "    # Remove self-connections (first column)\n",
        "    indices = indices[:, 1:]  # Skip first neighbor (self)\n",
        "\n",
        "    # Create edge index\n",
        "    num_nodes = features_for_knn.shape[0]\n",
        "    row = np.repeat(np.arange(num_nodes), k)\n",
        "    col = indices.flatten()\n",
        "\n",
        "    edge_index = torch.from_numpy(\n",
        "        np.vstack([row, col])\n",
        "    ).long().to(device)\n",
        "\n",
        "    timings['knn_computation'] = time.perf_counter() - t0\n",
        "    print(f\"  ‚úÖ PyNNDescent KNN computation: {timings['knn_computation']:.2f}s\")\n",
        "    print(f\"  üí° Speedup vs sklearn (est ~5min @ 20%): ~{300/timings['knn_computation']:.1f}x!\")\n",
        "\n",
        "    timings['edge_index_creation'] = 0.0  # Already included in knn_computation\n",
        "    print(f\"Edge index shape: {edge_index.shape}\")\n",
        "\n",
        "    # Add homogeneous coordinate (projective)\n",
        "    t0 = time.perf_counter()\n",
        "    node_features_uhg = torch.cat([\n",
        "        node_features.to(device),\n",
        "        torch.ones(node_features.size(0), 1, device=device)\n",
        "    ], dim=1)\n",
        "    timings['add_homogeneous'] = time.perf_counter() - t0\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    node_features_uhg = projective_normalize(node_features_uhg)\n",
        "    timings['projective_normalize'] = time.perf_counter() - t0\n",
        "    print(f\"  ‚è±Ô∏è  UHG projection: {timings['projective_normalize']:.2f}s\")\n",
        "\n",
        "    print(f\"Feature shape with homogeneous coordinate: {node_features_uhg.shape}\")\n",
        "\n",
        "    # Verify UHG constraints\n",
        "    t0 = time.perf_counter()\n",
        "    verify_uhg_constraints(node_features_uhg, name=\"initial features\")\n",
        "    timings['constraint_verification'] = time.perf_counter() - t0\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    total_samples = len(node_features_uhg)\n",
        "    indices_split = torch.randperm(total_samples)\n",
        "    train_size = int(0.7 * total_samples)\n",
        "    val_size = int(0.15 * total_samples)\n",
        "\n",
        "    train_mask = torch.zeros(total_samples, dtype=torch.bool, device=device)\n",
        "    val_mask = torch.zeros(total_samples, dtype=torch.bool, device=device)\n",
        "    test_mask = torch.zeros(total_samples, dtype=torch.bool, device=device)\n",
        "\n",
        "    train_mask[indices_split[:train_size]] = True\n",
        "    val_mask[indices_split[train_size:train_size+val_size]] = True\n",
        "    test_mask[indices_split[train_size+val_size:]] = True\n",
        "    timings['split_creation'] = time.perf_counter() - t0\n",
        "\n",
        "    print(f\"\\nTrain size: {train_mask.sum()}, Val size: {val_mask.sum()}, Test size: {test_mask.sum()}\")\n",
        "\n",
        "    timings['total'] = sum(timings.values())\n",
        "    print(f\"\\n‚è±Ô∏è  Total graph construction time: {timings['total']:.2f}s\")\n",
        "\n",
        "    return Data(\n",
        "        x=node_features_uhg,\n",
        "        edge_index=edge_index,\n",
        "        y=labels.to(device),\n",
        "        train_mask=train_mask,\n",
        "        val_mask=val_mask,\n",
        "        test_mask=test_mask\n",
        "    ).to(device), timings\n",
        "\n",
        "# ==============================\n",
        "# UHG GraphSAGE Message Passing\n",
        "# ==============================\n",
        "\n",
        "from torch_scatter import scatter_add\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "\n",
        "class UHGMessagePassing(MessagePassing):\n",
        "    def __init__(self, in_features: int, out_features: int):\n",
        "        super().__init__(aggr='add')\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight_msg = nn.Parameter(torch.Tensor(in_features, out_features))\n",
        "        self.weight_node = nn.Parameter(torch.Tensor(in_features, out_features))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.weight_msg)\n",
        "        nn.init.xavier_uniform_(self.weight_node)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
        "        # x includes homogeneous coord\n",
        "        # Transform node features (spatial only)\n",
        "        features = x[:, :-1]\n",
        "        z = x[:, -1:]\n",
        "        transformed_features = features @ self.weight_node\n",
        "        # Propagate using full projective vectors for weight computation\n",
        "        # Pass explicit size to handle all nodes (including isolated ones)\n",
        "        out = self.propagate(edge_index, x=x, size=(x.size(0), x.size(0)))\n",
        "        # Combine\n",
        "        out = out + transformed_features\n",
        "        # Recompute time-like to maintain Minkowski norm -1\n",
        "        out_full = torch.cat([out, z], dim=1)\n",
        "        out_full = projective_normalize(out_full)\n",
        "        return out_full\n",
        "\n",
        "    def message(self, x_i: torch.Tensor, x_j: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
        "        # x_i, x_j are full projective vectors\n",
        "        weights = torch.exp(-uhg_quadrance_vectorized(x_i, x_j))\n",
        "        # Transform neighbor features (spatial only)\n",
        "        messages = (x_j[:, :-1]) @ self.weight_msg\n",
        "        return messages * weights.view(-1, 1)\n",
        "\n",
        "    def aggregate(self, inputs: torch.Tensor, index: torch.Tensor, ptr=None, dim_size=None) -> torch.Tensor:\n",
        "        # Sum messages per destination (with explicit dim_size to handle all nodes)\n",
        "        numerator = scatter_add(inputs, index, dim=0, dim_size=dim_size)\n",
        "        # Sum weights per destination (approximate by ones per feature dim)\n",
        "        weights_sum = scatter_add(torch.ones_like(inputs), index, dim=0, dim_size=dim_size)\n",
        "        return numerator / torch.clamp(weights_sum, min=1e-6)\n",
        "\n",
        "class UHGGraphSAGE(nn.Module):\n",
        "    def __init__(self, in_channels: int, hidden_channels: int, out_channels: int, num_layers: int, dropout: float = 0.2):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # in_channels includes homogeneous coord\n",
        "        actual_in = in_channels - 1\n",
        "        self.layers.append(UHGMessagePassing(actual_in, hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.layers.append(UHGMessagePassing(hidden_channels, hidden_channels))\n",
        "        self.layers.append(UHGMessagePassing(hidden_channels, out_channels))\n",
        "\n",
        "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
        "        h = x\n",
        "        for layer in self.layers[:-1]:\n",
        "            h = layer(h, edge_index)\n",
        "            # Apply ReLU on spatial part only\n",
        "            spatial = F.relu(h[:, :-1])\n",
        "            h = torch.cat([spatial, h[:, -1:]], dim=1)\n",
        "            h = self.dropout(h)\n",
        "        h = self.layers[-1](h, edge_index)\n",
        "        return h[:, :-1]  # logits on spatial part\n",
        "\n",
        "# =====================\n",
        "# Training / Evaluation\n",
        "# =====================\n",
        "\n",
        "def train_epoch(model: nn.Module, graph_data: Data, optimizer: torch.optim.Optimizer, criterion: nn.Module, detailed_timing: bool = False) -> Tuple[float, dict]:\n",
        "    \"\"\"Train one epoch with optional detailed timing\"\"\"\n",
        "    model.train()\n",
        "    timings = {}\n",
        "\n",
        "    try:\n",
        "        t0 = time.perf_counter()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        if detailed_timing:\n",
        "            timings['zero_grad'] = time.perf_counter() - t0\n",
        "\n",
        "        # Single full-batch forward/backward on the static graph\n",
        "        t0 = time.perf_counter()\n",
        "        out = model(graph_data.x, graph_data.edge_index)\n",
        "        if detailed_timing:\n",
        "            timings['forward_pass'] = time.perf_counter() - t0\n",
        "\n",
        "        t0 = time.perf_counter()\n",
        "        loss = criterion(out[graph_data.train_mask], graph_data.y[graph_data.train_mask])\n",
        "        if detailed_timing:\n",
        "            timings['loss_computation'] = time.perf_counter() - t0\n",
        "\n",
        "        t0 = time.perf_counter()\n",
        "        loss.backward()\n",
        "        if detailed_timing:\n",
        "            timings['backward_pass'] = time.perf_counter() - t0\n",
        "\n",
        "        t0 = time.perf_counter()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        if detailed_timing:\n",
        "            timings['grad_clipping'] = time.perf_counter() - t0\n",
        "\n",
        "        t0 = time.perf_counter()\n",
        "        optimizer.step()\n",
        "        if detailed_timing:\n",
        "            timings['optimizer_step'] = time.perf_counter() - t0\n",
        "            timings['total'] = sum(timings.values())\n",
        "\n",
        "        return float(loss.item()), timings\n",
        "    except Exception as e:\n",
        "        print(f\"Train step failure: {e}\")\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model: nn.Module, graph_data: Data, mask: torch.Tensor, detailed_timing: bool = False) -> Tuple[float, dict]:\n",
        "    \"\"\"Evaluate with optional detailed timing\"\"\"\n",
        "    timings = {}\n",
        "    model.eval()\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    out = model(graph_data.x, graph_data.edge_index)\n",
        "    if detailed_timing:\n",
        "        timings['forward_pass'] = time.perf_counter() - t0\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    pred = out[mask].argmax(dim=1)\n",
        "    acc = (pred == graph_data.y[mask]).float().mean().item()\n",
        "    if detailed_timing:\n",
        "        timings['prediction'] = time.perf_counter() - t0\n",
        "        timings['total'] = sum(timings.values())\n",
        "\n",
        "    return acc, timings\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_detailed(model: nn.Module, graph_data: Data, mask: torch.Tensor, label_mapping: dict, phase: str = \"Test\") -> dict:\n",
        "    \"\"\"Detailed per-class evaluation (fixed for missing classes)\"\"\"\n",
        "    model.eval()\n",
        "    out = model(graph_data.x, graph_data.edge_index)\n",
        "    pred = out[mask].argmax(dim=1).cpu().numpy()\n",
        "    true = graph_data.y[mask].cpu().numpy()\n",
        "\n",
        "    # Reverse label mapping\n",
        "    idx_to_label = {v: k for k, v in label_mapping.items()}\n",
        "\n",
        "    # Only include classes that actually appear in test set\n",
        "    unique_classes = np.unique(np.concatenate([true, pred]))\n",
        "    target_names = [idx_to_label[i] for i in unique_classes]\n",
        "\n",
        "    # Show which classes are missing\n",
        "    all_classes = set(range(len(label_mapping)))\n",
        "    present_classes = set(unique_classes)\n",
        "    missing_classes = all_classes - present_classes\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"{phase} Set - Detailed Performance Report\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    if missing_classes:\n",
        "        print(f\"\\n‚ö†Ô∏è  WARNING: {len(missing_classes)} classes not present in {phase.lower()} set:\")\n",
        "        for class_idx in sorted(missing_classes):\n",
        "            print(f\"  ‚Ä¢ {idx_to_label[class_idx]}\")\n",
        "        print(f\"  (This is normal with small sample sizes and rare classes)\")\n",
        "\n",
        "    # Overall accuracy\n",
        "    overall_acc = (pred == true).mean()\n",
        "    print(f\"\\nOverall Accuracy: {overall_acc:.4f}\")\n",
        "    print(f\"Classes evaluated: {len(unique_classes)}/{len(label_mapping)}\")\n",
        "\n",
        "    # Per-class metrics (only for classes present in test set)\n",
        "    print(\"\\nPer-Class Classification Report:\")\n",
        "    print(classification_report(true, pred, labels=unique_classes, target_names=target_names, zero_division=0, digits=4))\n",
        "\n",
        "    # Confusion matrix (abbreviated)\n",
        "    cm = confusion_matrix(true, pred)\n",
        "    print(\"\\nPer-Class Accuracy:\")\n",
        "    for i, label in enumerate(target_names):\n",
        "        class_acc = cm[i, i] / cm[i].sum() if cm[i].sum() > 0 else 0.0\n",
        "        class_samples = cm[i].sum()\n",
        "        print(f\"  {label:30s}: {class_acc:.4f} ({int(class_samples)} samples)\")\n",
        "\n",
        "    # Macro and weighted F1\n",
        "    f1_macro = f1_score(true, pred, average='macro', zero_division=0)\n",
        "    f1_weighted = f1_score(true, pred, average='weighted', zero_division=0)\n",
        "    print(f\"\\nF1 Score (Macro):    {f1_macro:.4f}\")\n",
        "    print(f\"\\nF1 Score (Weighted): {f1_weighted:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'accuracy': float(overall_acc),\n",
        "        'f1_macro': float(f1_macro),\n",
        "        'f1_weighted': float(f1_weighted),\n",
        "        'confusion_matrix': cm.tolist(),\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    run_started = time.perf_counter()\n",
        "    run_id = datetime.now().strftime('%Y%m%dT%H%M%S')\n",
        "    metrics = {\n",
        "        'version': 'v4.8.2',\n",
        "        'weighting_strategy': WEIGHTING_STRATEGY,\n",
        "        'run_id': run_id,\n",
        "        'env': get_env_info(),\n",
        "        'paths': {\n",
        "            'file_path': FILE_PATH,\n",
        "            'model_save_path': MODEL_SAVE_PATH,\n",
        "            'results_path': RESULTS_PATH,\n",
        "        },\n",
        "        'improvements': [\n",
        "            f'v4.8.2: DAMPENED class weights ({WEIGHTING_STRATEGY}) for precision fix!',\n",
        "            'v4.8.2: PyNNDescent KNN (proven: 3x faster, <2% accuracy loss)',\n",
        "            'v4.8.2: 20% data sampling (566k samples) - Max for L4 GPU!',\n",
        "            f'v4.8.2: {WEIGHTING_STRATEGY} weighting to balance recall vs precision',\n",
        "            'v4.8.2: Target: 93-96% accuracy, 50-90% precision, 90-95% recall',\n",
        "            'PyNNDescent for 10-50x faster KNN',\n",
        "            'PCA dimensionality reduction for faster KNN (77 ‚Üí 20 dims)',\n",
        "            'k=2 neighbors (optimized for speed)',\n",
        "            'Detailed per-class metrics (fixed for missing classes)',\n",
        "            'UHG constraint verification',\n",
        "            'Comprehensive timing instrumentation',\n",
        "            'GPU detection and memory tracking',\n",
        "        ],\n",
        "        'data': {},\n",
        "        'graph': {},\n",
        "        'model': {},\n",
        "        'train': {\n",
        "            'epochs': [],\n",
        "            'best_val': 0.0,\n",
        "            'best_epoch': None,\n",
        "        },\n",
        "        'errors': None,\n",
        "        'timing': {},\n",
        "        'gpu_memory': {},\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Data loading with detailed timing (20% for L4 GPU, WITH dampened class weighting)\n",
        "        node_features, labels, label_mapping, class_weights, data_timings = load_and_preprocess_data(\n",
        "            FILE_PATH,\n",
        "            sample_frac=0.20,\n",
        "            weighting_strategy=WEIGHTING_STRATEGY\n",
        "        )\n",
        "\n",
        "        metrics['data'] = {\n",
        "            'num_nodes': int(node_features.size(0)),\n",
        "            'num_features': int(node_features.size(1)),\n",
        "            'num_classes': int(len(label_mapping)),\n",
        "            'sample_fraction': 0.20,\n",
        "            'memory_optimized': True,\n",
        "            'max_for_l4_gpu': True,\n",
        "            'class_weighted_loss': True,\n",
        "            'weighting_strategy': WEIGHTING_STRATEGY,\n",
        "        }\n",
        "        metrics['timing']['data_load'] = data_timings\n",
        "\n",
        "        # Graph construction with detailed timing, PCA, and PyNNDescent\n",
        "        graph_data, graph_timings = create_graph_data(node_features, labels, k=2, use_pca=True, pca_components=20)\n",
        "\n",
        "        metrics['timing']['graph_build'] = graph_timings\n",
        "        metrics['graph'] = {\n",
        "            'num_nodes': int(graph_data.x.size(0)),\n",
        "            'num_edges': int(graph_data.edge_index.size(1)),\n",
        "            'k_neighbors': 2,\n",
        "            'pca_enabled': True,\n",
        "            'pca_components': 20,\n",
        "            'knn_method': 'pynndescent',\n",
        "            'knn_approximate': True,\n",
        "            'train_nodes': int(graph_data.train_mask.sum().item()),\n",
        "            'val_nodes': int(graph_data.val_mask.sum().item()),\n",
        "            'test_nodes': int(graph_data.test_mask.sum().item()),\n",
        "        }\n",
        "\n",
        "        in_channels = graph_data.x.size(1)\n",
        "        hidden_channels = 64\n",
        "        out_channels = len(label_mapping)\n",
        "        num_layers = 2\n",
        "\n",
        "        model = UHGGraphSAGE(in_channels, hidden_channels, out_channels, num_layers).to(device)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
        "\n",
        "        # DAMPENED CLASS-WEIGHTED CrossEntropyLoss\n",
        "        criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "        print(f\"\\n‚úÖ Using DAMPENED CLASS-WEIGHTED CrossEntropyLoss\")\n",
        "        print(f\"   ‚Ä¢ Strategy: {WEIGHTING_STRATEGY.upper()}\")\n",
        "        print(f\"   ‚Ä¢ Goal: Fix precision collapse (1-2% ‚Üí 50-90%!)\")\n",
        "        print(f\"   ‚Ä¢ 20% data (566k samples) with PyNNDescent KNN\")\n",
        "\n",
        "        metrics['model'] = {\n",
        "            'in_channels': in_channels,\n",
        "            'hidden_channels': hidden_channels,\n",
        "            'out_channels': out_channels,\n",
        "            'num_layers': num_layers,\n",
        "            'class_weighted_loss': True,\n",
        "            'weighting_strategy': WEIGHTING_STRATEGY,\n",
        "        }\n",
        "\n",
        "        # Track GPU memory before training\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.reset_peak_memory_stats()\n",
        "            torch.cuda.empty_cache()\n",
        "            mem_allocated_before = torch.cuda.memory_allocated() / 1024**3\n",
        "            mem_reserved_before = torch.cuda.memory_reserved() / 1024**3\n",
        "            print(f\"\\nüíæ GPU Memory (before training):\")\n",
        "            print(f\"   ‚Ä¢ Allocated: {mem_allocated_before:.2f} GB\")\n",
        "            print(f\"   ‚Ä¢ Reserved:  {mem_reserved_before:.2f} GB\")\n",
        "\n",
        "        print(\"\\nStarting training...\")\n",
        "\n",
        "        best_val_acc = 0.0\n",
        "        best_epoch = 0\n",
        "        patience = 10\n",
        "        epochs_without_improvement = 0\n",
        "        max_epochs = 200\n",
        "\n",
        "        epoch_times = []\n",
        "        train_losses = []\n",
        "        val_accs = []\n",
        "        test_accs = []\n",
        "\n",
        "        for epoch in range(1, max_epochs + 1):\n",
        "            t_epoch_start = time.perf_counter()\n",
        "\n",
        "            # Detailed timing for epochs 1, 2, 50, 100\n",
        "            detailed = (epoch in [1, 2, 50, 100])\n",
        "\n",
        "            loss, train_timings = train_epoch(model, graph_data, optimizer, criterion, detailed_timing=detailed)\n",
        "            val_acc, val_timings = evaluate(model, graph_data, graph_data.val_mask, detailed_timing=detailed)\n",
        "            test_acc, test_timings = evaluate(model, graph_data, graph_data.test_mask, detailed_timing=detailed)\n",
        "\n",
        "            scheduler.step(val_acc)\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "            epoch_time = time.perf_counter() - t_epoch_start\n",
        "            epoch_times.append(epoch_time)\n",
        "            train_losses.append(loss)\n",
        "            val_accs.append(val_acc)\n",
        "            test_accs.append(test_acc)\n",
        "\n",
        "            improved = val_acc > best_val_acc\n",
        "            if improved:\n",
        "                best_val_acc = val_acc\n",
        "                best_epoch = epoch\n",
        "                epochs_without_improvement = 0\n",
        "                torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "                improved_str = \"(saved)\"\n",
        "            else:\n",
        "                epochs_without_improvement += 1\n",
        "                improved_str = \"\"\n",
        "\n",
        "            # Print detailed timing for specific epochs\n",
        "            if detailed:\n",
        "                print(f\"\\n‚è±Ô∏è  Epoch {epoch} Detailed Timing:\")\n",
        "                print(f\"    Train: Forward={train_timings.get('forward_pass', 0):.3f}s, \"\n",
        "                      f\"Backward={train_timings.get('backward_pass', 0):.3f}s, \"\n",
        "                      f\"Optimizer={train_timings.get('optimizer_step', 0):.3f}s\")\n",
        "                print(f\"    Val:   Forward={val_timings.get('forward_pass', 0):.3f}s\")\n",
        "                print(f\"    Test:  Forward={test_timings.get('forward_pass', 0):.3f}s\")\n",
        "\n",
        "            # Print every epoch if improved, or every 10 epochs\n",
        "            if improved or epoch % 10 == 0 or epoch == max_epochs:\n",
        "                print(f\"Epoch {epoch:03d} | Loss {loss:.4f} | Val {val_acc:.4f} | \"\n",
        "                      f\"Test {test_acc:.4f} | LR {current_lr:.5f} | {epoch_time:.2f}s | {improved_str}\")\n",
        "\n",
        "            if epochs_without_improvement >= patience:\n",
        "                print(f\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "        print(f\"\\n‚è±Ô∏è  Average epoch time: {np.mean(epoch_times):.2f}s\")\n",
        "\n",
        "        # Track GPU memory after training\n",
        "        if torch.cuda.is_available():\n",
        "            mem_allocated_peak = torch.cuda.max_memory_allocated() / 1024**3\n",
        "            mem_allocated_final = torch.cuda.memory_allocated() / 1024**3\n",
        "            mem_reserved_final = torch.cuda.memory_reserved() / 1024**3\n",
        "\n",
        "            print(f\"\\nüíæ GPU Memory Usage Summary:\")\n",
        "            print(f\"   ‚Ä¢ Peak Allocated: {mem_allocated_peak:.2f} GB\")\n",
        "            print(f\"   ‚Ä¢ Final Allocated: {mem_allocated_final:.2f} GB\")\n",
        "            print(f\"   ‚Ä¢ Final Reserved: {mem_reserved_final:.2f} GB\")\n",
        "\n",
        "            metrics['gpu_memory'] = {\n",
        "                'peak_allocated_gb': float(mem_allocated_peak),\n",
        "                'final_allocated_gb': float(mem_allocated_final),\n",
        "                'final_reserved_gb': float(mem_reserved_final),\n",
        "            }\n",
        "\n",
        "        metrics['train']['epochs'] = list(range(1, len(train_losses) + 1))\n",
        "        metrics['train']['losses'] = [float(x) for x in train_losses]\n",
        "        metrics['train']['val_accs'] = [float(x) for x in val_accs]\n",
        "        metrics['train']['test_accs'] = [float(x) for x in test_accs]\n",
        "        metrics['train']['best_val'] = float(best_val_acc)\n",
        "        metrics['train']['best_epoch'] = int(best_epoch)\n",
        "        metrics['train']['total_epochs'] = len(train_losses)\n",
        "        metrics['train']['avg_epoch_time'] = float(np.mean(epoch_times))\n",
        "\n",
        "        # Load best model and evaluate\n",
        "        print(\"\\nLoading best model for final evaluation...\")\n",
        "        model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "\n",
        "        # Verify UHG constraints after training\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"POST-TRAINING UHG CONSTRAINT VERIFICATION\")\n",
        "        print(\"=\"*80)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Check constraints after first layer\n",
        "            h = graph_data.x\n",
        "            h = model.layers[0](h, graph_data.edge_index)\n",
        "            verify_uhg_constraints(h, name=\"after layer 1\")\n",
        "\n",
        "        # Final test evaluation with detailed metrics\n",
        "        test_results = evaluate_detailed(model, graph_data, graph_data.test_mask, label_mapping, phase=\"Test\")\n",
        "        metrics['test'] = test_results\n",
        "\n",
        "        print(f\"\\nFinal Test Accuracy: {test_results['accuracy']:.4f}\")\n",
        "\n",
        "        # Comprehensive timing breakdown\n",
        "        run_ended = time.perf_counter()\n",
        "        total_runtime = run_ended - run_started\n",
        "\n",
        "        data_time = data_timings['total']\n",
        "        graph_time = graph_timings['total']\n",
        "        train_time = sum(epoch_times)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"‚è±Ô∏è  COMPREHENSIVE TIMING BREAKDOWN\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        print(f\"\\nüìä DATA LOADING ({data_time:.2f}s total):\")\n",
        "        for key, val in data_timings.items():\n",
        "            if key != 'total':\n",
        "                pct = (val / total_runtime) * 100\n",
        "                print(f\"  ‚Ä¢ {key.replace('_', ' ').title():20s} {val:7.2f}s ({pct:5.1f}%)\")\n",
        "\n",
        "        print(f\"\\nüï∏Ô∏è  GRAPH CONSTRUCTION ({graph_time:.2f}s total):\")\n",
        "        for key, val in graph_timings.items():\n",
        "            if key != 'total':\n",
        "                pct = (val / total_runtime) * 100\n",
        "                if key == 'pca':\n",
        "                    print(f\"  ‚Ä¢ PCA (77‚Üí20 dims): {val:7.2f}s ({pct:5.1f}%)\")\n",
        "                elif key == 'knn_computation':\n",
        "                    print(f\"  ‚Ä¢ KNN Computation: {val:7.2f}s ({pct:5.1f}%) üöÄ PyNNDescent\")\n",
        "                else:\n",
        "                    print(f\"  ‚Ä¢ {key.replace('_', ' ').title():20s} {val:7.2f}s ({pct:5.1f}%)\")\n",
        "\n",
        "        print(f\"\\nüéì TRAINING ({train_time:.2f}s total, {(train_time/total_runtime)*100:.1f}% of runtime):\")\n",
        "        print(f\"  ‚Ä¢ Avg Epoch Time:      {np.mean(epoch_times):.2f}s\")\n",
        "        print(f\"  ‚Ä¢ Total Epochs:      {len(epoch_times)}\")\n",
        "\n",
        "        print(f\"\\nüìà HIGH-LEVEL SUMMARY:\")\n",
        "        print(f\"  ‚Ä¢ Data Loading:       {(data_time/total_runtime)*100:5.1f}% of total runtime\")\n",
        "        print(f\"  ‚Ä¢ Graph Building:     {(graph_time/total_runtime)*100:5.1f}% of total runtime\")\n",
        "        print(f\"  ‚Ä¢ Training:           {(train_time/total_runtime)*100:5.1f}% of total runtime\")\n",
        "        print(f\"  ‚Ä¢ Total Runtime:     {total_runtime:7.2f}s ({total_runtime/60:.1f} min)\")\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"  ‚Ä¢ Peak GPU Memory:   {mem_allocated_peak:.2f} GB\")\n",
        "\n",
        "        print(f\"\\nüöÄ PYNNDESCENT PERFORMANCE:\")\n",
        "        print(f\"  ‚Ä¢ KNN Time (PyNNDescent): {graph_timings['knn_computation']:.0f}s\")\n",
        "        print(f\"  ‚Ä¢ Est. sklearn Time:      ~300s (5 min)\")\n",
        "        print(f\"  ‚Ä¢ Speedup:                ~{300/graph_timings['knn_computation']:.1f}x FASTER! üöÄ\")\n",
        "\n",
        "        print(f\"\\nüî¨ DAMPENED WEIGHTS ({WEIGHTING_STRATEGY.upper()}) RESULTS:\")\n",
        "        print(f\"  ‚Ä¢ Strategy: {WEIGHTING_STRATEGY}\")\n",
        "        print(f\"  ‚Ä¢ Test Accuracy: {test_results['accuracy']*100:.2f}%\")\n",
        "        print(f\"  ‚Ä¢ Macro F1: {test_results['f1_macro']:.4f}\")\n",
        "        print(f\"  ‚Ä¢ Weighted F1: {test_results['f1_weighted']:.4f}\")\n",
        "\n",
        "        # Bottleneck analysis\n",
        "        all_timings = []\n",
        "        for category, timing_dict in [('Data', data_timings), ('Graph', graph_timings)]:\n",
        "            for key, val in timing_dict.items():\n",
        "                if key != 'total' and val > 1.0:  # Only show > 1s\n",
        "                    all_timings.append((f\"{category}: {key}\", val))\n",
        "\n",
        "        all_timings.sort(key=lambda x: x[1], reverse=True)\n",
        "        print(f\"\\nüîç BOTTLENECK ANALYSIS:\")\n",
        "        for i, (name, time_val) in enumerate(all_timings[:3], 1):\n",
        "            pct = (time_val / total_runtime) * 100\n",
        "            print(f\"  {i}. {name:40s} {time_val:7.2f}s ({pct:5.1f}%)\")\n",
        "\n",
        "        metrics['timing']['total_runtime'] = float(total_runtime)\n",
        "        metrics['timing']['data_time'] = float(data_time)\n",
        "        metrics['timing']['graph_time'] = float(graph_time)\n",
        "        metrics['timing']['train_time'] = float(train_time)\n",
        "\n",
        "        # Save metrics\n",
        "        metrics_file = os.path.join(RESULTS_PATH, f'metrics_v4.8.2_{WEIGHTING_STRATEGY}_{run_id}.json')\n",
        "        with open(metrics_file, 'w') as f:\n",
        "            json.dump(metrics, f, indent=2)\n",
        "        print(f\"\\nSaved metrics to: {metrics_file}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"UHG IDS Model v4.8.2 ({WEIGHTING_STRATEGY.upper()}) - Training Complete\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"Results saved to: {metrics_file}\")\n",
        "        print(f\"\\nüî¨ DAMPENED CLASS WEIGHTING TEST SUCCESS!\")\n",
        "        print(f\"   ‚Ä¢ Strategy: {WEIGHTING_STRATEGY.upper()}\")\n",
        "        print(f\"   ‚Ä¢ Total samples: {metrics['data']['num_nodes']:,}\")\n",
        "        print(f\"   ‚Ä¢ KNN time: {graph_timings['knn_computation']:.0f}s\")\n",
        "        print(f\"   ‚Ä¢ Total time: {total_runtime:.0f}s ({total_runtime/60:.1f} min)\")\n",
        "        print(f\"   ‚Ä¢ Accuracy: {test_results['accuracy']*100:.2f}%\")\n",
        "        print(f\"   ‚Ä¢ Macro F1: {test_results['f1_macro']:.4f}\")\n",
        "        print(f\"\\nüéØ Run this 3 times with different WEIGHTING_STRATEGY values:\")\n",
        "        print(f\"   1. WEIGHTING_STRATEGY = 'sqrt'\")\n",
        "        print(f\"   2. WEIGHTING_STRATEGY = 'log'\")\n",
        "        print(f\"   3. WEIGHTING_STRATEGY = 'capped'\")\n",
        "        print(f\"   Then compare results to find optimal balance!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Training failed with error: {e}\")\n",
        "        traceback.print_exc()\n",
        "        metrics['errors'] = str(e)\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5u-EUOuImZuN",
        "outputId": "b6d9f273-2685-4201-b900-3084245ec141"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "\n",
            "================================================================================\n",
            "üñ•Ô∏è  HARDWARE CONFIGURATION\n",
            "================================================================================\n",
            "‚úÖ GPU Detected:\n",
            "   ‚Ä¢ Model: NVIDIA L4\n",
            "   ‚Ä¢ Memory: 22.2 GB\n",
            "   ‚Ä¢ CUDA Version: 12.6\n",
            "   ‚Ä¢ Compute Capability: 8.9\n",
            "   ‚Ä¢ Device: cuda:0\n",
            "\n",
            "üî¨ Configuration (v4.8.2 - Dampened Class Weights):\n",
            "   ‚Ä¢ KNN Method: PyNNDescent (Approximate) ‚≠ê PROVEN!\n",
            "   ‚Ä¢ Loss: DAMPENED CLASS-WEIGHTED CrossEntropyLoss ‚öñÔ∏è\n",
            "   ‚Ä¢ Weighting Strategy: SQRT üî¨\n",
            "   ‚Ä¢ Data: 20% sampling (566k samples) - Max for L4 GPU!\n",
            "   ‚Ä¢ Expected KNN time: ~10-15s (vs ~70s sklearn!)\n",
            "   ‚Ä¢ Expected total time: ~2-3 minutes\n",
            "   ‚Ä¢ Goal: Fix precision collapse (1-2% ‚Üí 50-90%!)\n",
            "================================================================================\n",
            "\n",
            "üìä SQRT DAMPENING:\n",
            "   ‚Ä¢ Formula: weight = sqrt(total / (num_classes * count))\n",
            "   ‚Ä¢ Effect: Moderate dampening (9435 ‚Üí 97)\n",
            "   ‚Ä¢ Best for: 10-100x class imbalance\n",
            "   ‚Ä¢ Expected: 93-95% accuracy, 50-70% precision, 95%+ recall\n",
            "\n",
            "\n",
            "Loading data from: /content/drive/MyDrive/CIC_data.csv\n",
            "  ‚è±Ô∏è  CSV read: 33.54s\n",
            "\n",
            "Unique labels in the dataset: ['BENIGN' 'DDoS' 'PortScan' 'Bot' 'Infiltration'\n",
            " 'Web Attack ÔøΩ Brute Force' 'Web Attack ÔøΩ XSS'\n",
            " 'Web Attack ÔøΩ Sql Injection' 'FTP-Patator' 'SSH-Patator' 'DoS slowloris'\n",
            " 'DoS Slowhttptest' 'DoS Hulk' 'DoS GoldenEye' 'Heartbleed']\n",
            "\n",
            "Label distribution in the dataset:\n",
            "Label\n",
            "BENIGN                        2273097\n",
            "DoS Hulk                       231073\n",
            "PortScan                       158930\n",
            "DDoS                           128027\n",
            "DoS GoldenEye                   10293\n",
            "FTP-Patator                      7938\n",
            "SSH-Patator                      5897\n",
            "DoS slowloris                    5796\n",
            "DoS Slowhttptest                 5499\n",
            "Bot                              1966\n",
            "Web Attack ÔøΩ Brute Force         1507\n",
            "Web Attack ÔøΩ XSS                  652\n",
            "Infiltration                       36\n",
            "Web Attack ÔøΩ Sql Injection         21\n",
            "Heartbleed                         11\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Applying random sampling (frac=0.2)...\n",
            "  ‚è±Ô∏è  Sampling: 0.56s\n",
            "\n",
            "Sampled label distribution:\n",
            "Label\n",
            "BENIGN                        454434\n",
            "DoS Hulk                       46486\n",
            "PortScan                       31711\n",
            "DDoS                           25583\n",
            "DoS GoldenEye                   2074\n",
            "FTP-Patator                     1612\n",
            "DoS slowloris                   1173\n",
            "SSH-Patator                     1173\n",
            "DoS Slowhttptest                1072\n",
            "Bot                              390\n",
            "Web Attack ÔøΩ Brute Force         308\n",
            "Web Attack ÔøΩ XSS                 119\n",
            "Web Attack ÔøΩ Sql Injection         7\n",
            "Infiltration                       5\n",
            "Heartbleed                         2\n",
            "Name: count, dtype: int64\n",
            "  ‚è±Ô∏è  Convert to numeric: 0.85s\n",
            "  ‚è±Ô∏è  Fill NaN/inf: 1.91s\n",
            "  ‚è±Ô∏è  Scaling: 0.75s\n",
            "  ‚è±Ô∏è  Convert to tensors: 0.02s\n",
            "\n",
            "Preprocessing complete.\n",
            "Feature shape: torch.Size([566149, 77])\n",
            "Number of unique labels: 15\n",
            "\n",
            "Class distribution in processed data:\n",
            "  BENIGN                        :  454434 samples (80.27%)\n",
            "  Bot                           :     390 samples ( 0.07%)\n",
            "  DDoS                          :   25583 samples ( 4.52%)\n",
            "  DoS GoldenEye                 :    2074 samples ( 0.37%)\n",
            "  DoS Hulk                      :   46486 samples ( 8.21%)\n",
            "  DoS Slowhttptest              :    1072 samples ( 0.19%)\n",
            "  DoS slowloris                 :    1173 samples ( 0.21%)\n",
            "  FTP-Patator                   :    1612 samples ( 0.28%)\n",
            "  Heartbleed                    :       2 samples ( 0.00%)\n",
            "  Infiltration                  :       5 samples ( 0.00%)\n",
            "  PortScan                      :   31711 samples ( 5.60%)\n",
            "  SSH-Patator                   :    1173 samples ( 0.21%)\n",
            "  Web Attack ÔøΩ Brute Force      :     308 samples ( 0.05%)\n",
            "  Web Attack ÔøΩ Sql Injection    :       7 samples ( 0.00%)\n",
            "  Web Attack ÔøΩ XSS              :     119 samples ( 0.02%)\n",
            "\n",
            "‚öñÔ∏è  Computing Dampened Class Weights (SQRT):\n",
            "   ‚Ä¢ Strategy: SQRT dampening\n",
            "   ‚Ä¢ Formula: sqrt(total / (num_classes * count))\n",
            "\n",
            "   Weights comparison (Dampened vs Raw Inverse):\n",
            "   Class                            Dampened        Raw  Reduction\n",
            "   -----------------------------------------------------------------\n",
            "   BENIGN                             0.2882        0.1    -247.0%\n",
            "   Bot                                9.8376       96.8      89.8%\n",
            "   DDoS                               1.2146        1.5      17.7%\n",
            "   DoS GoldenEye                      4.2659       18.2      76.6%\n",
            "   DoS Hulk                           0.9011        0.8     -11.0%\n",
            "   DoS Slowhttptest                   5.9337       35.2      83.1%\n",
            "   DoS slowloris                      5.6725       32.2      82.4%\n",
            "   FTP-Patator                        4.8388       23.4      79.3%\n",
            "   Heartbleed                       137.3741    18871.6      99.3%\n",
            "   Infiltration                      86.8830     7548.7      98.8%\n",
            "   PortScan                           1.0910        1.2       8.3%\n",
            "   SSH-Patator                        5.6725       32.2      82.4%\n",
            "   Web Attack ÔøΩ Brute Force          11.0699      122.5      91.0%\n",
            "   Web Attack ÔøΩ Sql Injection        73.4295     5391.9      98.6%\n",
            "   Web Attack ÔøΩ XSS                  17.8093      317.2      94.4%\n",
            "\n",
            "‚è±Ô∏è  Total data loading time: 38.05s\n",
            "\n",
            "Creating graph structure...\n",
            "\n",
            "Applying PCA for faster KNN...\n",
            "  ‚Ä¢ Original features: 77\n",
            "  ‚Ä¢ Reduced features: 20\n",
            "  ‚Ä¢ Explained variance: 0.8971 (89.71%)\n",
            "  ‚è±Ô∏è  PCA: 0.19s\n",
            "\n",
            "üöÄ Computing KNN graph with PyNNDescent (k=2)...\n",
            "  ‚Ä¢ Input shape: (566149, 20)\n",
            "  ‚Ä¢ Number of samples: 566,149\n",
            "  ‚Ä¢ Number of features: 20\n",
            "  ‚Ä¢ Using PyNNDescent (approximate)\n",
            "  ‚Ä¢ Algorithm: Nearest Neighbor Descent\n",
            "  ‚Ä¢ Expected: 10-20x faster than sklearn!\n",
            "  ‚Ä¢ Accuracy: 95-98% of exact KNN\n",
            "  ‚Ä¢ Building NNDescent index...\n",
            "  ‚úÖ PyNNDescent KNN computation: 26.49s\n",
            "  üí° Speedup vs sklearn (est ~5min @ 20%): ~11.3x!\n",
            "Edge index shape: torch.Size([2, 1132298])\n",
            "  ‚è±Ô∏è  UHG projection: 0.14s\n",
            "Feature shape with homogeneous coordinate: torch.Size([566149, 78])\n",
            "UHG Constraint Check (initial features):\n",
            "  Max violation: 0.062500\n",
            "  Mean violation: 0.000003\n",
            "  ‚ö†Ô∏è WARNING: Constraints violated!\n",
            "\n",
            "Train size: 396304, Val size: 84922, Test size: 84923\n",
            "\n",
            "‚è±Ô∏è  Total graph construction time: 27.01s\n",
            "\n",
            "‚úÖ Using DAMPENED CLASS-WEIGHTED CrossEntropyLoss\n",
            "   ‚Ä¢ Strategy: SQRT\n",
            "   ‚Ä¢ Goal: Fix precision collapse (1-2% ‚Üí 50-90%!)\n",
            "   ‚Ä¢ 20% data (566k samples) with PyNNDescent KNN\n",
            "\n",
            "üíæ GPU Memory (before training):\n",
            "   ‚Ä¢ Allocated: 0.19 GB\n",
            "   ‚Ä¢ Reserved:  0.21 GB\n",
            "\n",
            "Starting training...\n",
            "\n",
            "‚è±Ô∏è  Epoch 1 Detailed Timing:\n",
            "    Train: Forward=0.232s, Backward=0.435s, Optimizer=0.117s\n",
            "    Val:   Forward=0.003s\n",
            "    Test:  Forward=0.002s\n",
            "Epoch 001 | Loss 3.6116 | Val 0.5631 | Test 0.5625 | LR 0.01000 | 1.15s | (saved)\n",
            "\n",
            "‚è±Ô∏è  Epoch 2 Detailed Timing:\n",
            "    Train: Forward=0.003s, Backward=0.004s, Optimizer=0.001s\n",
            "    Val:   Forward=0.003s\n",
            "    Test:  Forward=0.002s\n",
            "Epoch 002 | Loss 2.3445 | Val 0.8036 | Test 0.7994 | LR 0.01000 | 0.40s | (saved)\n",
            "Epoch 003 | Loss 1.5672 | Val 0.8614 | Test 0.8604 | LR 0.01000 | 0.40s | (saved)\n",
            "Epoch 004 | Loss 1.2438 | Val 0.8820 | Test 0.8819 | LR 0.01000 | 0.40s | (saved)\n",
            "Epoch 005 | Loss 1.0779 | Val 0.8975 | Test 0.8979 | LR 0.01000 | 0.40s | (saved)\n",
            "Epoch 006 | Loss 0.9577 | Val 0.8999 | Test 0.8993 | LR 0.01000 | 0.40s | (saved)\n",
            "Epoch 010 | Loss 0.6998 | Val 0.8752 | Test 0.8775 | LR 0.01000 | 0.40s | \n",
            "Epoch 013 | Loss 0.5920 | Val 0.9020 | Test 0.9032 | LR 0.00500 | 0.40s | (saved)\n",
            "Epoch 014 | Loss 0.5781 | Val 0.9101 | Test 0.9105 | LR 0.00500 | 0.40s | (saved)\n",
            "Epoch 015 | Loss 0.5612 | Val 0.9187 | Test 0.9198 | LR 0.00500 | 0.40s | (saved)\n",
            "Epoch 018 | Loss 0.5203 | Val 0.9209 | Test 0.9205 | LR 0.00500 | 0.40s | (saved)\n",
            "Epoch 019 | Loss 0.5155 | Val 0.9216 | Test 0.9213 | LR 0.00500 | 0.40s | (saved)\n",
            "Epoch 020 | Loss 0.4970 | Val 0.9233 | Test 0.9226 | LR 0.00500 | 0.40s | (saved)\n",
            "Epoch 023 | Loss 0.4662 | Val 0.9254 | Test 0.9247 | LR 0.00500 | 0.40s | (saved)\n",
            "Epoch 024 | Loss 0.4555 | Val 0.9272 | Test 0.9262 | LR 0.00500 | 0.40s | (saved)\n",
            "Epoch 025 | Loss 0.4519 | Val 0.9291 | Test 0.9281 | LR 0.00500 | 0.40s | (saved)\n",
            "Epoch 026 | Loss 0.4436 | Val 0.9293 | Test 0.9287 | LR 0.00500 | 0.40s | (saved)\n",
            "Epoch 029 | Loss 0.4192 | Val 0.9298 | Test 0.9306 | LR 0.00500 | 0.40s | (saved)\n",
            "Epoch 030 | Loss 0.4121 | Val 0.9320 | Test 0.9324 | LR 0.00500 | 0.40s | (saved)\n",
            "Epoch 031 | Loss 0.4042 | Val 0.9335 | Test 0.9338 | LR 0.00500 | 0.40s | (saved)\n",
            "Epoch 032 | Loss 0.3987 | Val 0.9351 | Test 0.9351 | LR 0.00500 | 0.40s | (saved)\n",
            "Epoch 033 | Loss 0.3934 | Val 0.9371 | Test 0.9372 | LR 0.00500 | 0.40s | (saved)\n",
            "Epoch 034 | Loss 0.3903 | Val 0.9379 | Test 0.9382 | LR 0.00500 | 0.40s | (saved)\n",
            "Epoch 035 | Loss 0.3826 | Val 0.9409 | Test 0.9407 | LR 0.00500 | 0.40s | (saved)\n",
            "Epoch 040 | Loss 0.3569 | Val 0.9381 | Test 0.9382 | LR 0.00500 | 0.40s | \n",
            "Epoch 043 | Loss 0.3468 | Val 0.9419 | Test 0.9418 | LR 0.00250 | 0.40s | (saved)\n",
            "Epoch 044 | Loss 0.3441 | Val 0.9425 | Test 0.9427 | LR 0.00250 | 0.40s | (saved)\n",
            "Epoch 045 | Loss 0.3426 | Val 0.9429 | Test 0.9429 | LR 0.00250 | 0.40s | (saved)\n",
            "Epoch 046 | Loss 0.3406 | Val 0.9433 | Test 0.9431 | LR 0.00250 | 0.40s | (saved)\n",
            "Epoch 047 | Loss 0.3375 | Val 0.9433 | Test 0.9430 | LR 0.00250 | 0.40s | (saved)\n",
            "Epoch 048 | Loss 0.3382 | Val 0.9435 | Test 0.9432 | LR 0.00250 | 0.40s | (saved)\n",
            "Epoch 049 | Loss 0.3316 | Val 0.9442 | Test 0.9439 | LR 0.00250 | 0.40s | (saved)\n",
            "\n",
            "‚è±Ô∏è  Epoch 50 Detailed Timing:\n",
            "    Train: Forward=0.003s, Backward=0.004s, Optimizer=0.000s\n",
            "    Val:   Forward=0.003s\n",
            "    Test:  Forward=0.002s\n",
            "Epoch 050 | Loss 0.3326 | Val 0.9451 | Test 0.9446 | LR 0.00250 | 0.40s | (saved)\n",
            "Epoch 051 | Loss 0.3309 | Val 0.9452 | Test 0.9446 | LR 0.00250 | 0.40s | (saved)\n",
            "Epoch 060 | Loss 0.3129 | Val 0.9433 | Test 0.9435 | LR 0.00125 | 0.40s | \n",
            "Early stopping.\n",
            "\n",
            "‚è±Ô∏è  Average epoch time: 0.41s\n",
            "\n",
            "üíæ GPU Memory Usage Summary:\n",
            "   ‚Ä¢ Peak Allocated: 3.10 GB\n",
            "   ‚Ä¢ Final Allocated: 0.20 GB\n",
            "   ‚Ä¢ Final Reserved: 3.91 GB\n",
            "\n",
            "Loading best model for final evaluation...\n",
            "\n",
            "================================================================================\n",
            "POST-TRAINING UHG CONSTRAINT VERIFICATION\n",
            "================================================================================\n",
            "UHG Constraint Check (after layer 1):\n",
            "  Max violation: 1.000000\n",
            "  Mean violation: 0.000023\n",
            "  ‚ö†Ô∏è WARNING: Constraints violated!\n",
            "\n",
            "================================================================================\n",
            "Test Set - Detailed Performance Report\n",
            "================================================================================\n",
            "\n",
            "Overall Accuracy: 0.9446\n",
            "Classes evaluated: 15/15\n",
            "\n",
            "Per-Class Classification Report:\n",
            "                            precision    recall  f1-score   support\n",
            "\n",
            "                    BENIGN     0.9858    0.9482    0.9666     68279\n",
            "                       Bot     0.0000    0.0000    0.0000        48\n",
            "                      DDoS     0.9392    0.9445    0.9418      3858\n",
            "             DoS GoldenEye     0.6734    0.8933    0.7679       300\n",
            "                  DoS Hulk     0.8751    0.8950    0.8849      6826\n",
            "          DoS Slowhttptest     0.5841    0.8408    0.6893       157\n",
            "             DoS slowloris     0.6881    0.7943    0.7374       175\n",
            "               FTP-Patator     0.3084    0.9303    0.4633       244\n",
            "                Heartbleed     0.0000    0.0000    0.0000         1\n",
            "              Infiltration     0.0000    0.0000    0.0000         0\n",
            "                  PortScan     0.7829    0.9929    0.8755      4783\n",
            "               SSH-Patator     0.3858    0.9255    0.5446       188\n",
            "  Web Attack ÔøΩ Brute Force     0.1262    0.7917    0.2178        48\n",
            "Web Attack ÔøΩ Sql Injection     0.0000    0.0000    0.0000         1\n",
            "          Web Attack ÔøΩ XSS     0.0000    0.0000    0.0000        15\n",
            "\n",
            "                  accuracy                         0.9446     84923\n",
            "                 macro avg     0.4233    0.5971    0.4726     84923\n",
            "              weighted avg     0.9564    0.9446    0.9486     84923\n",
            "\n",
            "\n",
            "Per-Class Accuracy:\n",
            "  BENIGN                        : 0.9482 (68279 samples)\n",
            "  Bot                           : 0.0000 (48 samples)\n",
            "  DDoS                          : 0.9445 (3858 samples)\n",
            "  DoS GoldenEye                 : 0.8933 (300 samples)\n",
            "  DoS Hulk                      : 0.8950 (6826 samples)\n",
            "  DoS Slowhttptest              : 0.8408 (157 samples)\n",
            "  DoS slowloris                 : 0.7943 (175 samples)\n",
            "  FTP-Patator                   : 0.9303 (244 samples)\n",
            "  Heartbleed                    : 0.0000 (1 samples)\n",
            "  Infiltration                  : 0.0000 (0 samples)\n",
            "  PortScan                      : 0.9929 (4783 samples)\n",
            "  SSH-Patator                   : 0.9255 (188 samples)\n",
            "  Web Attack ÔøΩ Brute Force      : 0.7917 (48 samples)\n",
            "  Web Attack ÔøΩ Sql Injection    : 0.0000 (1 samples)\n",
            "  Web Attack ÔøΩ XSS              : 0.0000 (15 samples)\n",
            "\n",
            "F1 Score (Macro):    0.4726\n",
            "\n",
            "F1 Score (Weighted): 0.9486\n",
            "\n",
            "Final Test Accuracy: 0.9446\n",
            "\n",
            "================================================================================\n",
            "‚è±Ô∏è  COMPREHENSIVE TIMING BREAKDOWN\n",
            "================================================================================\n",
            "\n",
            "üìä DATA LOADING (38.05s total):\n",
            "  ‚Ä¢ Csv Read               33.54s ( 36.1%)\n",
            "  ‚Ä¢ Column Cleanup          0.43s (  0.5%)\n",
            "  ‚Ä¢ Sampling                0.56s (  0.6%)\n",
            "  ‚Ä¢ To Numeric              0.85s (  0.9%)\n",
            "  ‚Ä¢ Fillna                  1.91s (  2.0%)\n",
            "  ‚Ä¢ Scaling                 0.75s (  0.8%)\n",
            "  ‚Ä¢ To Tensors              0.02s (  0.0%)\n",
            "\n",
            "üï∏Ô∏è  GRAPH CONSTRUCTION (27.01s total):\n",
            "  ‚Ä¢ To Numpy                0.00s (  0.0%)\n",
            "  ‚Ä¢ PCA (77‚Üí20 dims):    0.19s (  0.2%)\n",
            "  ‚Ä¢ KNN Computation:   26.49s ( 28.5%) üöÄ PyNNDescent\n",
            "  ‚Ä¢ Edge Index Creation     0.00s (  0.0%)\n",
            "  ‚Ä¢ Add Homogeneous         0.07s (  0.1%)\n",
            "  ‚Ä¢ Projective Normalize    0.14s (  0.2%)\n",
            "  ‚Ä¢ Constraint Verification    0.07s (  0.1%)\n",
            "  ‚Ä¢ Split Creation          0.04s (  0.0%)\n",
            "\n",
            "üéì TRAINING (25.23s total, 27.1% of runtime):\n",
            "  ‚Ä¢ Avg Epoch Time:      0.41s\n",
            "  ‚Ä¢ Total Epochs:      61\n",
            "\n",
            "üìà HIGH-LEVEL SUMMARY:\n",
            "  ‚Ä¢ Data Loading:        40.9% of total runtime\n",
            "  ‚Ä¢ Graph Building:      29.0% of total runtime\n",
            "  ‚Ä¢ Training:            27.1% of total runtime\n",
            "  ‚Ä¢ Total Runtime:       92.97s (1.5 min)\n",
            "  ‚Ä¢ Peak GPU Memory:   3.10 GB\n",
            "\n",
            "üöÄ PYNNDESCENT PERFORMANCE:\n",
            "  ‚Ä¢ KNN Time (PyNNDescent): 26s\n",
            "  ‚Ä¢ Est. sklearn Time:      ~300s (5 min)\n",
            "  ‚Ä¢ Speedup:                ~11.3x FASTER! üöÄ\n",
            "\n",
            "üî¨ DAMPENED WEIGHTS (SQRT) RESULTS:\n",
            "  ‚Ä¢ Strategy: sqrt\n",
            "  ‚Ä¢ Test Accuracy: 94.46%\n",
            "  ‚Ä¢ Macro F1: 0.4726\n",
            "  ‚Ä¢ Weighted F1: 0.9486\n",
            "\n",
            "üîç BOTTLENECK ANALYSIS:\n",
            "  1. Data: csv_read                             33.54s ( 36.1%)\n",
            "  2. Graph: knn_computation                     26.49s ( 28.5%)\n",
            "  3. Data: fillna                                1.91s (  2.0%)\n",
            "\n",
            "Saved metrics to: /content/drive/MyDrive/uhg_ids_results/metrics_v4.8.2_sqrt_20251004T132224.json\n",
            "\n",
            "================================================================================\n",
            "UHG IDS Model v4.8.2 (SQRT) - Training Complete\n",
            "================================================================================\n",
            "Results saved to: /content/drive/MyDrive/uhg_ids_results/metrics_v4.8.2_sqrt_20251004T132224.json\n",
            "\n",
            "üî¨ DAMPENED CLASS WEIGHTING TEST SUCCESS!\n",
            "   ‚Ä¢ Strategy: SQRT\n",
            "   ‚Ä¢ Total samples: 566,149\n",
            "   ‚Ä¢ KNN time: 26s\n",
            "   ‚Ä¢ Total time: 93s (1.5 min)\n",
            "   ‚Ä¢ Accuracy: 94.46%\n",
            "   ‚Ä¢ Macro F1: 0.4726\n",
            "\n",
            "üéØ Run this 3 times with different WEIGHTING_STRATEGY values:\n",
            "   1. WEIGHTING_STRATEGY = 'sqrt'\n",
            "   2. WEIGHTING_STRATEGY = 'log'\n",
            "   3. WEIGHTING_STRATEGY = 'capped'\n",
            "   Then compare results to find optimal balance!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PyUd2tJAmf8-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}