{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMJ7hk98MB0ytObS01OuDCu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zbovaird/UHG-Models/blob/main/uhg_intrusion_detection5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PyTorch Geometric (matches your current torch/cuda)\n",
        "!pip -q install --upgrade pip\n",
        "import torch\n",
        "pt = torch.__version__.split('+')[0]\n",
        "cuda = torch.version.cuda\n",
        "if torch.cuda.is_available() and cuda:\n",
        "  idx = f\"https://data.pyg.org/whl/torch-{pt}+cu{cuda.replace('.','')}.html\"\n",
        "else:\n",
        "  idx = f\"https://data.pyg.org/whl/torch-{pt}+cpu.html\"\n",
        "\n",
        "!pip -q install torch_scatter torch_sparse torch_cluster torch_spline_conv -f {idx}\n",
        "!pip -q install torch_geometric scikit-learn scipy pandas tqdm"
      ],
      "metadata": {
        "id": "kQzUM7wXk01Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a28a6518-d1ea-4a8b-8add-47bb96a65bdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.8 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "Intrusion Detection using Universal Hyperbolic Geometry (UHG) - GPT5 v2\n",
        "Based on original GPT5 training approach with comprehensive v4.5 metrics added.\n",
        "\n",
        "Configuration (Matched to v4.5 for Direct Comparison):\n",
        "- 10% data sampling (matching v4.5)\n",
        "- k=2 neighbors (matching v4.5)\n",
        "- No class weights (vs v4.5 with class weights)\n",
        "- No PCA (vs v4.5 with PCA 77‚Üí20 dims)\n",
        "- Standard cross-entropy loss (vs v4.5 with weighted loss)\n",
        "- n_jobs=-1 for KNN (vs v4.5 with n_jobs=4)\n",
        "\n",
        "This configuration isolates the impact of:\n",
        "1. Class weighting\n",
        "2. PCA dimensionality reduction\n",
        "\n",
        "v2 Additions:\n",
        "- Comprehensive timing instrumentation and bottleneck analysis\n",
        "- GPU hardware detection and memory tracking\n",
        "- Detailed per-class evaluation with classification report\n",
        "- UHG constraint verification (initial and post-training)\n",
        "- Enhanced progress reporting\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "from scipy.sparse import coo_matrix\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torch_geometric.data import Data\n",
        "from typing import Tuple\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "import traceback\n",
        "import platform\n",
        "from datetime import datetime\n",
        "\n",
        "# Optional: Drive mount (only in Colab)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    print(\"Mounting Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Device configuration with detailed GPU info\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üñ•Ô∏è  HARDWARE CONFIGURATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB\n",
        "    cuda_version = torch.version.cuda\n",
        "    gpu_capability = torch.cuda.get_device_capability(0)\n",
        "\n",
        "    print(f\"‚úÖ GPU Detected:\")\n",
        "    print(f\"   ‚Ä¢ Model: {gpu_name}\")\n",
        "    print(f\"   ‚Ä¢ Memory: {gpu_memory:.1f} GB\")\n",
        "    print(f\"   ‚Ä¢ CUDA Version: {cuda_version}\")\n",
        "    print(f\"   ‚Ä¢ Compute Capability: {gpu_capability[0]}.{gpu_capability[1]}\")\n",
        "    print(f\"   ‚Ä¢ Device: cuda:0\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  No GPU available - using CPU\")\n",
        "    print(f\"   ‚Ä¢ This will be significantly slower for training\")\n",
        "\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# File paths (same as base)\n",
        "FILE_PATH = '/content/drive/MyDrive/CIC_data.csv'\n",
        "MODEL_SAVE_PATH = '/content/drive/MyDrive/uhg_ids_model_gpt5_v2.pth'\n",
        "RESULTS_PATH = '/content/drive/MyDrive/uhg_ids_results'\n",
        "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
        "\n",
        "# ========================\n",
        "# Metrics/Env helpers\n",
        "# ========================\n",
        "\n",
        "def get_env_info() -> dict:\n",
        "    info = {\n",
        "        'python': sys.version.split()[0],\n",
        "        'platform': platform.platform(),\n",
        "        'torch': torch.__version__,\n",
        "        'cuda_available': torch.cuda.is_available(),\n",
        "        'cudnn_enabled': torch.backends.cudnn.enabled,\n",
        "    }\n",
        "    if torch.cuda.is_available():\n",
        "        info.update({\n",
        "            'gpu_name': torch.cuda.get_device_name(0),\n",
        "            'cuda_capability': torch.cuda.get_device_capability(0),\n",
        "            'cuda_version': torch.version.cuda,\n",
        "        })\n",
        "    return info\n",
        "\n",
        "def save_json(obj: dict, path: str) -> None:\n",
        "    try:\n",
        "        with open(path, 'w') as f:\n",
        "            json.dump(obj, f, indent=2)\n",
        "        print(f\"Saved metrics to: {path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to save metrics JSON: {e}\")\n",
        "\n",
        "# ===============\n",
        "# UHG primitives\n",
        "# ===============\n",
        "\n",
        "def minkowski_dot(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Minkowski inner product with J=diag(1,1,-1) for final dim = 3 or D.\n",
        "    ‚ü®x,y‚ü© = Œ£_i x_i y_i (i < D) ‚àí x_D y_D\n",
        "    \"\"\"\n",
        "    return (x[..., :-1] * y[..., :-1]).sum(dim=-1) - x[..., -1] * y[..., -1]\n",
        "\n",
        "def projective_normalize(points: torch.Tensor, eps: float = 1e-9) -> torch.Tensor:\n",
        "    \"\"\"Ensure last coordinate is time-like such that x^2 + y^2 ‚àí z^2 = ‚àí1.\n",
        "    Recompute z = sqrt(1 + ||spatial||^2) for stability.\n",
        "    \"\"\"\n",
        "    spatial = points[..., :-1]\n",
        "    time_like = torch.sqrt(torch.clamp(1.0 + (spatial * spatial).sum(dim=-1, keepdim=True), min=eps))\n",
        "    return torch.cat([spatial, time_like], dim=-1)\n",
        "\n",
        "def uhg_quadrance_vectorized(x: torch.Tensor, y: torch.Tensor, eps: float = 1e-9) -> torch.Tensor:\n",
        "    \"\"\"Projective quadrance q = 1 ‚àí (‚ü®x,y‚ü©^2) / (‚ü®x,x‚ü©‚ü®y,y‚ü©). Stable and batched.\"\"\"\n",
        "    xx = minkowski_dot(x, x)\n",
        "    yy = minkowski_dot(y, y)\n",
        "    xy = minkowski_dot(x, y)\n",
        "    denom = torch.clamp(xx * yy, min=eps)\n",
        "    q = 1.0 - (xy * xy) / denom\n",
        "    return torch.clamp(q, 0.0, 1.0)\n",
        "\n",
        "def uhg_spread_vectorized(L: torch.Tensor, M: torch.Tensor, eps: float = 1e-9) -> torch.Tensor:\n",
        "    \"\"\"Spread between lines: S = 1 ‚àí (‚ü®L,M‚ü©^2) / (‚ü®L,L‚ü©‚ü®M,M‚ü©). Stable and batched.\"\"\"\n",
        "    LL = minkowski_dot(L, L)\n",
        "    MM = minkowski_dot(M, M)\n",
        "    LM = minkowski_dot(L, M)\n",
        "    denom = torch.clamp(LL * MM, min=eps)\n",
        "    s = 1.0 - (LM * LM) / denom\n",
        "    return torch.clamp(s, 0.0, 1.0)\n",
        "\n",
        "def verify_uhg_constraints(x: torch.Tensor, eps: float = 1e-3, name: str = \"features\") -> bool:\n",
        "    \"\"\"Verify Minkowski norm constraint: x¬≤ + y¬≤ - z¬≤ = -1\"\"\"\n",
        "    spatial_norm = (x[:, :-1] ** 2).sum(dim=-1)\n",
        "    time_norm = x[:, -1] ** 2\n",
        "    minkowski_norm = spatial_norm - time_norm\n",
        "\n",
        "    # Should be -1 for all points\n",
        "    violation = torch.abs(minkowski_norm + 1.0)\n",
        "    max_violation = violation.max().item()\n",
        "    mean_violation = violation.mean().item()\n",
        "\n",
        "    print(f\"UHG Constraint Check ({name}):\")\n",
        "    print(f\"  Max violation: {max_violation:.6f}\")\n",
        "    print(f\"  Mean violation: {mean_violation:.6f}\")\n",
        "\n",
        "    if max_violation > eps:\n",
        "        print(f\"  ‚ö†Ô∏è WARNING: Constraints violated!\")\n",
        "        return False\n",
        "    else:\n",
        "        print(f\"  ‚úÖ Constraints satisfied\")\n",
        "        return True\n",
        "\n",
        "# ==============================\n",
        "# Data loading / preprocessing\n",
        "# ==============================\n",
        "\n",
        "def load_and_preprocess_data(file_path: str = FILE_PATH, sample_frac: float = 0.10) -> Tuple[torch.Tensor, torch.Tensor, dict, dict]:\n",
        "    \"\"\"Load and preprocess data with detailed timing measurements\"\"\"\n",
        "    timings = {}\n",
        "\n",
        "    print(f\"\\nLoading data from: {file_path}\")\n",
        "    t0 = time.perf_counter()\n",
        "    data = pd.read_csv(file_path, low_memory=False)\n",
        "    timings['csv_read'] = time.perf_counter() - t0\n",
        "    print(f\"  ‚è±Ô∏è  CSV read: {timings['csv_read']:.2f}s\")\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    data.columns = data.columns.str.strip()\n",
        "    data['Label'] = data['Label'].str.strip()\n",
        "    timings['column_cleanup'] = time.perf_counter() - t0\n",
        "\n",
        "    unique_labels = data['Label'].unique()\n",
        "    print(f\"\\nUnique labels in the dataset: {unique_labels}\")\n",
        "    label_counts = data['Label'].value_counts()\n",
        "    print(\"\\nLabel distribution in the dataset:\")\n",
        "    print(label_counts)\n",
        "\n",
        "    # Sample data (10% - matching v4.5 for comparison)\n",
        "    print(f\"\\nApplying random sampling (frac={sample_frac})...\")\n",
        "    t0 = time.perf_counter()\n",
        "    data_sampled = data.sample(frac=sample_frac, random_state=42)\n",
        "    timings['sampling'] = time.perf_counter() - t0\n",
        "    print(f\"  ‚è±Ô∏è  Sampling: {timings['sampling']:.2f}s\")\n",
        "\n",
        "    print(f\"\\nSampled label distribution:\")\n",
        "    sampled_label_counts = data_sampled['Label'].value_counts()\n",
        "    print(sampled_label_counts)\n",
        "\n",
        "    # Convert to numeric and handle missing values\n",
        "    t0 = time.perf_counter()\n",
        "    data_numeric = data_sampled.apply(pd.to_numeric, errors='coerce')\n",
        "    timings['to_numeric'] = time.perf_counter() - t0\n",
        "    print(f\"  ‚è±Ô∏è  Convert to numeric: {timings['to_numeric']:.2f}s\")\n",
        "\n",
        "    # Fill NaN and inf\n",
        "    t0 = time.perf_counter()\n",
        "    data_filled = data_numeric.fillna(data_numeric.mean())\n",
        "    data_filled = data_filled.replace([np.inf, -np.inf], np.nan)\n",
        "    data_filled = data_filled.fillna(data_filled.max())\n",
        "    if data_filled.isnull().values.any():\n",
        "        data_filled = data_filled.fillna(0)\n",
        "    timings['fillna'] = time.perf_counter() - t0\n",
        "    print(f\"  ‚è±Ô∏è  Fill NaN/inf: {timings['fillna']:.2f}s\")\n",
        "\n",
        "    labels = data_sampled['Label']\n",
        "    features = data_filled.drop(columns=['Label'])\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    scaler = StandardScaler()\n",
        "    features_scaled = scaler.fit_transform(features)\n",
        "    timings['scaling'] = time.perf_counter() - t0\n",
        "    print(f\"  ‚è±Ô∏è  Scaling: {timings['scaling']:.2f}s\")\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    node_features = torch.tensor(features_scaled, dtype=torch.float32)\n",
        "    label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "    labels_numeric = labels.map(label_mapping).values\n",
        "    labels_tensor = torch.tensor(labels_numeric, dtype=torch.long)\n",
        "    timings['to_tensor'] = time.perf_counter() - t0\n",
        "    print(f\"  ‚è±Ô∏è  Convert to tensors: {timings['to_tensor']:.2f}s\")\n",
        "\n",
        "    print(\"\\nPreprocessing complete.\")\n",
        "    print(f\"Feature shape: {node_features.shape}\")\n",
        "    print(f\"Number of unique labels: {len(unique_labels)}\")\n",
        "\n",
        "    timings['total'] = sum(timings.values())\n",
        "    print(f\"\\n‚è±Ô∏è  Total data loading time: {timings['total']:.2f}s\")\n",
        "\n",
        "    return node_features, labels_tensor, label_mapping, timings\n",
        "\n",
        "# =========================\n",
        "# Graph construction (KNN)\n",
        "# =========================\n",
        "\n",
        "def create_graph_data(node_features: torch.Tensor, labels: torch.Tensor, k: int = 2) -> Tuple[Data, dict]:\n",
        "    \"\"\"Original GPT5: k=2, no PCA, n_jobs=-1\"\"\"\n",
        "    timings = {}\n",
        "\n",
        "    print(\"\\nCreating graph structure...\")\n",
        "    t0 = time.perf_counter()\n",
        "    features_np = node_features.cpu().numpy()\n",
        "    timings['to_numpy'] = time.perf_counter() - t0\n",
        "\n",
        "    print(f\"\\nComputing KNN graph with k={k}...\")\n",
        "    print(f\"  ‚Ä¢ Input shape: {features_np.shape}\")\n",
        "    print(f\"  ‚Ä¢ Number of samples: {features_np.shape[0]:,}\")\n",
        "    print(f\"  ‚Ä¢ Number of features: {features_np.shape[1]}\")\n",
        "    print(f\"  ‚Ä¢ Using n_jobs=-1 (all CPU cores)\")\n",
        "    print(f\"  ‚Ä¢ No PCA (77 features - vs v4.5 with 20 features)\")\n",
        "    print(f\"  ‚Ä¢ Expected: Slower KNN than v4.5 due to higher dimensionality\")\n",
        "\n",
        "    import sys\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    knn_graph = kneighbors_graph(\n",
        "        features_np,\n",
        "        k,\n",
        "        mode='connectivity',\n",
        "        include_self=False,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    timings['knn_computation'] = time.perf_counter() - t0\n",
        "    print(f\"  ‚úÖ KNN computation: {timings['knn_computation']:.2f}s\")\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    knn_graph_coo = coo_matrix(knn_graph)\n",
        "    edge_index = torch.from_numpy(\n",
        "        np.vstack((knn_graph_coo.row, knn_graph_coo.col))\n",
        "    ).long().to(device)\n",
        "    timings['edge_index_creation'] = time.perf_counter() - t0\n",
        "    print(f\"  ‚è±Ô∏è  Edge index creation: {timings['edge_index_creation']:.2f}s\")\n",
        "\n",
        "    print(f\"Edge index shape: {edge_index.shape}\")\n",
        "\n",
        "    # Add homogeneous coordinate (projective)\n",
        "    t0 = time.perf_counter()\n",
        "    node_features_uhg = torch.cat([\n",
        "        node_features.to(device),\n",
        "        torch.ones(node_features.size(0), 1, device=device)\n",
        "    ], dim=1)\n",
        "    timings['add_homogeneous'] = time.perf_counter() - t0\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    node_features_uhg = projective_normalize(node_features_uhg)\n",
        "    timings['projective_normalize'] = time.perf_counter() - t0\n",
        "    print(f\"  ‚è±Ô∏è  UHG projection: {timings['projective_normalize']:.2f}s\")\n",
        "\n",
        "    print(f\"Feature shape with homogeneous coordinate: {node_features_uhg.shape}\")\n",
        "\n",
        "    # Verify UHG constraints\n",
        "    t0 = time.perf_counter()\n",
        "    verify_uhg_constraints(node_features_uhg, name=\"initial features\")\n",
        "    timings['constraint_verification'] = time.perf_counter() - t0\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    total_samples = len(node_features_uhg)\n",
        "    indices = torch.randperm(total_samples)\n",
        "    train_size = int(0.7 * total_samples)\n",
        "    val_size = int(0.15 * total_samples)\n",
        "\n",
        "    train_mask = torch.zeros(total_samples, dtype=torch.bool, device=device)\n",
        "    val_mask = torch.zeros(total_samples, dtype=torch.bool, device=device)\n",
        "    test_mask = torch.zeros(total_samples, dtype=torch.bool, device=device)\n",
        "\n",
        "    train_mask[indices[:train_size]] = True\n",
        "    val_mask[indices[train_size:train_size+val_size]] = True\n",
        "    test_mask[indices[train_size+val_size:]] = True\n",
        "    timings['split_creation'] = time.perf_counter() - t0\n",
        "\n",
        "    print(f\"\\nTrain size: {train_mask.sum()}, Val size: {val_mask.sum()}, Test size: {test_mask.sum()}\")\n",
        "\n",
        "    timings['total'] = sum(timings.values())\n",
        "    print(f\"\\n‚è±Ô∏è  Total graph construction time: {timings['total']:.2f}s\")\n",
        "\n",
        "    return Data(\n",
        "        x=node_features_uhg,\n",
        "        edge_index=edge_index,\n",
        "        y=labels.to(device),\n",
        "        train_mask=train_mask,\n",
        "        val_mask=val_mask,\n",
        "        test_mask=test_mask\n",
        "    ).to(device), timings\n",
        "\n",
        "# ==============================\n",
        "# UHG GraphSAGE Message Passing\n",
        "# ==============================\n",
        "\n",
        "from torch_scatter import scatter_add\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "\n",
        "class UHGMessagePassing(MessagePassing):\n",
        "    def __init__(self, in_features: int, out_features: int):\n",
        "        super().__init__(aggr='add')\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight_msg = nn.Parameter(torch.Tensor(in_features, out_features))\n",
        "        self.weight_node = nn.Parameter(torch.Tensor(in_features, out_features))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.weight_msg)\n",
        "        nn.init.xavier_uniform_(self.weight_node)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
        "        # x includes homogeneous coord\n",
        "        # Transform node features (spatial only)\n",
        "        features = x[:, :-1]\n",
        "        z = x[:, -1:]\n",
        "        transformed_features = features @ self.weight_node\n",
        "        # Propagate using full projective vectors for weight computation\n",
        "        out = self.propagate(edge_index, x=x, size=None)\n",
        "        # Combine\n",
        "        out = out + transformed_features\n",
        "        # Recompute time-like to maintain Minkowski norm -1\n",
        "        out_full = torch.cat([out, z], dim=1)\n",
        "        out_full = projective_normalize(out_full)\n",
        "        return out_full\n",
        "\n",
        "    def message(self, x_i: torch.Tensor, x_j: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
        "        # x_i, x_j are full projective vectors\n",
        "        weights = torch.exp(-uhg_quadrance_vectorized(x_i, x_j))\n",
        "        # Transform neighbor features (spatial only)\n",
        "        messages = (x_j[:, :-1]) @ self.weight_msg\n",
        "        return messages * weights.view(-1, 1)\n",
        "\n",
        "    def aggregate(self, inputs: torch.Tensor, index: torch.Tensor) -> torch.Tensor:\n",
        "        # Sum messages per destination\n",
        "        numerator = scatter_add(inputs, index, dim=0)\n",
        "        # Sum weights per destination (approximate by ones per feature dim)\n",
        "        weights_sum = scatter_add(torch.ones_like(inputs), index, dim=0)\n",
        "        return numerator / torch.clamp(weights_sum, min=1e-6)\n",
        "\n",
        "class UHGGraphSAGE(nn.Module):\n",
        "    def __init__(self, in_channels: int, hidden_channels: int, out_channels: int, num_layers: int, dropout: float = 0.2):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # in_channels includes homogeneous coord\n",
        "        actual_in = in_channels - 1\n",
        "        self.layers.append(UHGMessagePassing(actual_in, hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.layers.append(UHGMessagePassing(hidden_channels, hidden_channels))\n",
        "        self.layers.append(UHGMessagePassing(hidden_channels, out_channels))\n",
        "\n",
        "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
        "        h = x\n",
        "        for layer in self.layers[:-1]:\n",
        "            h = layer(h, edge_index)\n",
        "            # Apply ReLU on spatial part only\n",
        "            spatial = F.relu(h[:, :-1])\n",
        "            h = torch.cat([spatial, h[:, -1:]], dim=1)\n",
        "            h = self.dropout(h)\n",
        "        h = self.layers[-1](h, edge_index)\n",
        "        return h[:, :-1]  # logits on spatial part\n",
        "\n",
        "# =====================\n",
        "# Training / Evaluation\n",
        "# =====================\n",
        "\n",
        "def train_epoch(model: nn.Module, graph_data: Data, optimizer: torch.optim.Optimizer, criterion: nn.Module, detailed_timing: bool = False) -> Tuple[float, dict]:\n",
        "    \"\"\"Train one epoch with optional detailed timing\"\"\"\n",
        "    model.train()\n",
        "    timings = {}\n",
        "\n",
        "    try:\n",
        "        t0 = time.perf_counter()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        if detailed_timing:\n",
        "            timings['zero_grad'] = time.perf_counter() - t0\n",
        "\n",
        "        # Single full-batch forward/backward on the static graph\n",
        "        t0 = time.perf_counter()\n",
        "        out = model(graph_data.x, graph_data.edge_index)\n",
        "        if detailed_timing:\n",
        "            timings['forward_pass'] = time.perf_counter() - t0\n",
        "\n",
        "        t0 = time.perf_counter()\n",
        "        loss = criterion(out[graph_data.train_mask], graph_data.y[graph_data.train_mask])\n",
        "        if detailed_timing:\n",
        "            timings['loss_computation'] = time.perf_counter() - t0\n",
        "\n",
        "        t0 = time.perf_counter()\n",
        "        loss.backward()\n",
        "        if detailed_timing:\n",
        "            timings['backward_pass'] = time.perf_counter() - t0\n",
        "\n",
        "        t0 = time.perf_counter()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        if detailed_timing:\n",
        "            timings['grad_clipping'] = time.perf_counter() - t0\n",
        "\n",
        "        t0 = time.perf_counter()\n",
        "        optimizer.step()\n",
        "        if detailed_timing:\n",
        "            timings['optimizer_step'] = time.perf_counter() - t0\n",
        "            timings['total'] = sum(timings.values())\n",
        "\n",
        "        return float(loss.item()), timings\n",
        "    except Exception as e:\n",
        "        print(f\"Train step failure: {e}\")\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model: nn.Module, graph_data: Data, mask: torch.Tensor, detailed_timing: bool = False) -> Tuple[float, dict]:\n",
        "    \"\"\"Evaluate with optional detailed timing\"\"\"\n",
        "    timings = {}\n",
        "    model.eval()\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    out = model(graph_data.x, graph_data.edge_index)\n",
        "    if detailed_timing:\n",
        "        timings['forward_pass'] = time.perf_counter() - t0\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    pred = out[mask].argmax(dim=1)\n",
        "    acc = (pred == graph_data.y[mask]).float().mean().item()\n",
        "    if detailed_timing:\n",
        "        timings['prediction'] = time.perf_counter() - t0\n",
        "        timings['total'] = sum(timings.values())\n",
        "\n",
        "    return acc, timings\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_detailed(model: nn.Module, graph_data: Data, mask: torch.Tensor, label_mapping: dict, phase: str = \"Test\") -> dict:\n",
        "    \"\"\"Detailed per-class evaluation\"\"\"\n",
        "    model.eval()\n",
        "    out = model(graph_data.x, graph_data.edge_index)\n",
        "    pred = out[mask].argmax(dim=1).cpu().numpy()\n",
        "    true = graph_data.y[mask].cpu().numpy()\n",
        "\n",
        "    # Reverse label mapping\n",
        "    idx_to_label = {v: k for k, v in label_mapping.items()}\n",
        "\n",
        "    # Only include classes that actually appear in test set\n",
        "    unique_classes = np.unique(np.concatenate([true, pred]))\n",
        "    target_names = [idx_to_label[i] for i in unique_classes]\n",
        "\n",
        "    # Show which classes are missing\n",
        "    all_classes = set(range(len(label_mapping)))\n",
        "    present_classes = set(unique_classes)\n",
        "    missing_classes = all_classes - present_classes\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"{phase} Set - Detailed Performance Report\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    if missing_classes:\n",
        "        print(f\"\\n‚ö†Ô∏è  WARNING: {len(missing_classes)} classes not present in {phase.lower()} set:\")\n",
        "        for class_idx in sorted(missing_classes):\n",
        "            print(f\"  ‚Ä¢ {idx_to_label[class_idx]}\")\n",
        "        print(f\"  (This is normal with small sample sizes and rare classes)\")\n",
        "\n",
        "    # Overall accuracy\n",
        "    overall_acc = (pred == true).mean()\n",
        "    print(f\"\\nOverall Accuracy: {overall_acc:.4f}\")\n",
        "    print(f\"Classes evaluated: {len(unique_classes)}/{len(label_mapping)}\")\n",
        "\n",
        "    # Per-class metrics (only for classes present in test set)\n",
        "    print(\"\\nPer-Class Classification Report:\")\n",
        "    print(classification_report(true, pred, labels=unique_classes, target_names=target_names, zero_division=0, digits=4))\n",
        "\n",
        "    # Confusion matrix (abbreviated)\n",
        "    cm = confusion_matrix(true, pred)\n",
        "    print(\"\\nPer-Class Accuracy:\")\n",
        "    for i, label in enumerate(target_names):\n",
        "        class_acc = cm[i, i] / cm[i].sum() if cm[i].sum() > 0 else 0.0\n",
        "        class_samples = cm[i].sum()\n",
        "        print(f\"  {label:30s}: {class_acc:.4f} ({int(class_samples)} samples)\")\n",
        "\n",
        "    # Macro and weighted F1\n",
        "    f1_macro = f1_score(true, pred, average='macro', zero_division=0)\n",
        "    f1_weighted = f1_score(true, pred, average='weighted', zero_division=0)\n",
        "    print(f\"\\nF1 Score (Macro):    {f1_macro:.4f}\")\n",
        "    print(f\"F1 Score (Weighted): {f1_weighted:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'accuracy': float(overall_acc),\n",
        "        'f1_macro': float(f1_macro),\n",
        "        'f1_weighted': float(f1_weighted),\n",
        "        'confusion_matrix': cm.tolist(),\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    run_started = time.perf_counter()\n",
        "    run_id = datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')\n",
        "    metrics = {\n",
        "        'version': 'GPT5 v2',\n",
        "        'run_id': run_id,\n",
        "        'env': get_env_info(),\n",
        "        'paths': {\n",
        "            'file_path': FILE_PATH,\n",
        "            'model_save_path': MODEL_SAVE_PATH,\n",
        "            'results_path': RESULTS_PATH,\n",
        "        },\n",
        "        'configuration': [\n",
        "            'Original GPT5 training approach (modified for comparison)',\n",
        "            '10% data sampling (matching v4.5)',\n",
        "            'k=2 neighbors (matching v4.5)',\n",
        "            'No PCA dimensionality reduction',\n",
        "            'Standard cross-entropy loss (no class weighting)',\n",
        "            'n_jobs=-1 for KNN',\n",
        "        ],\n",
        "        'v2_additions': [\n",
        "            'Comprehensive timing instrumentation and bottleneck analysis',\n",
        "            'GPU hardware detection and memory tracking',\n",
        "            'Detailed per-class evaluation with classification report',\n",
        "            'UHG constraint verification (initial and post-training)',\n",
        "            'Enhanced progress reporting',\n",
        "        ],\n",
        "        'data': {},\n",
        "        'graph': {},\n",
        "        'model': {},\n",
        "        'train': {\n",
        "            'epochs': [],\n",
        "            'best_val': 0.0,\n",
        "            'best_epoch': None,\n",
        "        },\n",
        "        'errors': None,\n",
        "        'timing': {},\n",
        "        'gpu_memory': {},\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Data loading with detailed timing\n",
        "        node_features, labels, label_mapping, data_timings = load_and_preprocess_data(FILE_PATH, sample_frac=0.10)\n",
        "\n",
        "        metrics['data'] = {\n",
        "            'num_nodes': int(node_features.size(0)),\n",
        "            'num_features': int(node_features.size(1)),\n",
        "            'num_classes': int(len(label_mapping)),\n",
        "            'sample_fraction': 0.10,\n",
        "        }\n",
        "        metrics['timing']['data_load'] = data_timings\n",
        "\n",
        "        # Graph construction with detailed timing\n",
        "        graph_data, graph_timings = create_graph_data(node_features, labels, k=2)\n",
        "\n",
        "        metrics['timing']['graph_build'] = graph_timings\n",
        "        metrics['graph'] = {\n",
        "            'num_nodes': int(graph_data.x.size(0)),\n",
        "            'num_edges': int(graph_data.edge_index.size(1)),\n",
        "            'k_neighbors': 2,\n",
        "            'pca_enabled': False,\n",
        "            'train_nodes': int(graph_data.train_mask.sum().item()),\n",
        "            'val_nodes': int(graph_data.val_mask.sum().item()),\n",
        "            'test_nodes': int(graph_data.test_mask.sum().item()),\n",
        "        }\n",
        "\n",
        "        in_channels = graph_data.x.size(1)\n",
        "        hidden_channels = 64\n",
        "        out_channels = len(label_mapping)\n",
        "        num_layers = 2\n",
        "\n",
        "        model = UHGGraphSAGE(in_channels, hidden_channels, out_channels, num_layers).to(device)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
        "\n",
        "        # Standard cross-entropy loss (no class weighting - original GPT5)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        print(f\"\\n‚úÖ Using standard CrossEntropyLoss (no class weighting)\")\n",
        "\n",
        "        n_params = sum(p.numel() for p in model.parameters())\n",
        "        metrics['model'] = {\n",
        "            'hidden_channels': hidden_channels,\n",
        "            'out_channels': out_channels,\n",
        "            'num_layers': num_layers,\n",
        "            'num_parameters': int(n_params),\n",
        "            'class_weighted_loss': False,\n",
        "        }\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.reset_peak_memory_stats()\n",
        "            metrics['gpu_memory']['before_allocated'] = int(torch.cuda.memory_allocated())\n",
        "            metrics['gpu_memory']['before_reserved'] = int(torch.cuda.memory_reserved())\n",
        "            print(f\"\\nüíæ GPU Memory (before training):\")\n",
        "            print(f\"   ‚Ä¢ Allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
        "            print(f\"   ‚Ä¢ Reserved:  {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n",
        "\n",
        "        best_val = 0.0\n",
        "        patience = 20\n",
        "        no_improve = 0\n",
        "        num_epochs = 200\n",
        "\n",
        "        print(\"\\nStarting training...\")\n",
        "        train_start = time.perf_counter()\n",
        "\n",
        "        # Store detailed timing for specific epochs\n",
        "        detailed_timing_epochs = [1, 2, 50, 100]\n",
        "        epoch_timings_detailed = {}\n",
        "\n",
        "        for epoch in range(1, num_epochs + 1):\n",
        "            epoch_t0 = time.perf_counter()\n",
        "\n",
        "            # Enable detailed timing for specific epochs\n",
        "            detailed = epoch in detailed_timing_epochs\n",
        "\n",
        "            loss, train_timings = train_epoch(model, graph_data, optimizer, criterion, detailed_timing=detailed)\n",
        "            val_acc, val_timings = evaluate(model, graph_data, graph_data.val_mask, detailed_timing=detailed)\n",
        "            test_acc, test_timings = evaluate(model, graph_data, graph_data.test_mask, detailed_timing=detailed)\n",
        "\n",
        "            scheduler.step(val_acc)\n",
        "            lr = optimizer.param_groups[0]['lr']\n",
        "            epoch_time = time.perf_counter() - epoch_t0\n",
        "\n",
        "            epoch_metrics = {\n",
        "                'epoch': epoch,\n",
        "                'loss': float(loss),\n",
        "                'val_acc': float(val_acc),\n",
        "                'test_acc': float(test_acc),\n",
        "                'lr': float(lr),\n",
        "                'time_s': float(epoch_time),\n",
        "            }\n",
        "\n",
        "            # Store detailed timing breakdowns\n",
        "            if detailed:\n",
        "                epoch_timings_detailed[f'epoch_{epoch}'] = {\n",
        "                    'train': train_timings,\n",
        "                    'val': val_timings,\n",
        "                    'test': test_timings,\n",
        "                    'total': epoch_time,\n",
        "                }\n",
        "                print(f\"\\n‚è±Ô∏è  Epoch {epoch} Detailed Timing:\")\n",
        "                print(f\"    Train: Forward={train_timings.get('forward_pass', 0):.3f}s, Backward={train_timings.get('backward_pass', 0):.3f}s, Optimizer={train_timings.get('optimizer_step', 0):.3f}s\")\n",
        "                print(f\"    Val:   Forward={val_timings.get('forward_pass', 0):.3f}s\")\n",
        "                print(f\"    Test:  Forward={test_timings.get('forward_pass', 0):.3f}s\")\n",
        "\n",
        "            metrics['train']['epochs'].append(epoch_metrics)\n",
        "\n",
        "            if val_acc > best_val:\n",
        "                best_val = val_acc\n",
        "                no_improve = 0\n",
        "                metrics['train']['best_val'] = float(best_val)\n",
        "                metrics['train']['best_epoch'] = int(epoch)\n",
        "                torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "                print(f\"Epoch {epoch:03d} | Loss {loss:.4f} | Val {val_acc:.4f} | Test {test_acc:.4f} | LR {lr:.5f} | {epoch_time:.2f}s | (saved)\")\n",
        "            else:\n",
        "                no_improve += 1\n",
        "                if epoch % 10 == 0:\n",
        "                    print(f\"Epoch {epoch:03d} | Loss {loss:.4f} | Val {val_acc:.4f} | Test {test_acc:.4f} | LR {lr:.5f} | {epoch_time:.2f}s\")\n",
        "                if no_improve >= patience:\n",
        "                    print(\"Early stopping.\")\n",
        "                    break\n",
        "\n",
        "        train_total_time = time.perf_counter() - train_start\n",
        "        metrics['timing']['train_total_s'] = train_total_time\n",
        "        metrics['timing']['epoch_details'] = epoch_timings_detailed\n",
        "\n",
        "        # Calculate average epoch time\n",
        "        avg_epoch_time = train_total_time / epoch\n",
        "        print(f\"\\n‚è±Ô∏è  Average epoch time: {avg_epoch_time:.2f}s\")\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            metrics['gpu_memory']['after_allocated'] = int(torch.cuda.memory_allocated())\n",
        "            metrics['gpu_memory']['after_reserved'] = int(torch.cuda.memory_reserved())\n",
        "            metrics['gpu_memory']['peak_allocated'] = int(torch.cuda.max_memory_allocated())\n",
        "\n",
        "            print(f\"\\nüíæ GPU Memory Usage Summary:\")\n",
        "            print(f\"   ‚Ä¢ Peak Allocated: {torch.cuda.max_memory_allocated()/1024**3:.2f} GB\")\n",
        "            print(f\"   ‚Ä¢ Final Allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
        "            print(f\"   ‚Ä¢ Final Reserved: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n",
        "\n",
        "        if os.path.exists(MODEL_SAVE_PATH):\n",
        "            print(\"\\nLoading best model for final evaluation...\")\n",
        "            model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=device))\n",
        "\n",
        "            # Verify UHG constraints after training\n",
        "            print(\"\\n\" + \"=\"*80)\n",
        "            print(\"POST-TRAINING UHG CONSTRAINT VERIFICATION\")\n",
        "            print(\"=\"*80)\n",
        "            with torch.no_grad():\n",
        "                # Check features after one forward pass\n",
        "                h = model.layers[0](graph_data.x, graph_data.edge_index)\n",
        "                verify_uhg_constraints(h, name=\"after layer 1\")\n",
        "\n",
        "            # Detailed evaluation\n",
        "            final_metrics = evaluate_detailed(model, graph_data, graph_data.test_mask, label_mapping, phase=\"Test\")\n",
        "            metrics['train']['final_test_metrics'] = final_metrics\n",
        "            print(f\"\\nFinal Test Accuracy: {final_metrics['accuracy']:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        tb = traceback.format_exc()\n",
        "        print(\"\\nFATAL ERROR during run:\\n\", tb)\n",
        "        metrics['errors'] = {\n",
        "            'message': str(e),\n",
        "            'traceback': tb,\n",
        "        }\n",
        "    finally:\n",
        "        total_runtime = time.perf_counter() - run_started\n",
        "        metrics['timing']['total_s'] = total_runtime\n",
        "\n",
        "        # ===== COMPREHENSIVE TIMING SUMMARY =====\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"‚è±Ô∏è  COMPREHENSIVE TIMING BREAKDOWN\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        if 'data_load' in metrics['timing']:\n",
        "            data_t = metrics['timing']['data_load']\n",
        "            print(f\"\\nüìä DATA LOADING ({data_t.get('total', 0):.2f}s total):\")\n",
        "            print(f\"  ‚Ä¢ CSV Read:          {data_t.get('csv_read', 0):6.2f}s ({data_t.get('csv_read', 0)/total_runtime*100:5.1f}%)\")\n",
        "            print(f\"  ‚Ä¢ Sampling:          {data_t.get('sampling', 0):6.2f}s ({data_t.get('sampling', 0)/total_runtime*100:5.1f}%)\")\n",
        "            print(f\"  ‚Ä¢ To Numeric:        {data_t.get('to_numeric', 0):6.2f}s ({data_t.get('to_numeric', 0)/total_runtime*100:5.1f}%)\")\n",
        "            print(f\"  ‚Ä¢ Fill NaN/Inf:      {data_t.get('fillna', 0):6.2f}s ({data_t.get('fillna', 0)/total_runtime*100:5.1f}%)\")\n",
        "            print(f\"  ‚Ä¢ Scaling:           {data_t.get('scaling', 0):6.2f}s ({data_t.get('scaling', 0)/total_runtime*100:5.1f}%)\")\n",
        "            print(f\"  ‚Ä¢ To Tensors:        {data_t.get('to_tensor', 0):6.2f}s ({data_t.get('to_tensor', 0)/total_runtime*100:5.1f}%)\")\n",
        "\n",
        "        if 'graph_build' in metrics['timing']:\n",
        "            graph_t = metrics['timing']['graph_build']\n",
        "            print(f\"\\nüï∏Ô∏è  GRAPH CONSTRUCTION ({graph_t.get('total', 0):.2f}s total):\")\n",
        "            print(f\"  ‚Ä¢ KNN Computation:   {graph_t.get('knn_computation', 0):6.2f}s ({graph_t.get('knn_computation', 0)/total_runtime*100:5.1f}%)\")\n",
        "            print(f\"  ‚Ä¢ Edge Index:        {graph_t.get('edge_index_creation', 0):6.2f}s ({graph_t.get('edge_index_creation', 0)/total_runtime*100:5.1f}%)\")\n",
        "            print(f\"  ‚Ä¢ UHG Projection:    {graph_t.get('projective_normalize', 0):6.2f}s ({graph_t.get('projective_normalize', 0)/total_runtime*100:5.1f}%)\")\n",
        "            print(f\"  ‚Ä¢ Constraint Check:  {graph_t.get('constraint_verification', 0):6.2f}s ({graph_t.get('constraint_verification', 0)/total_runtime*100:5.1f}%)\")\n",
        "\n",
        "        if 'train_total_s' in metrics['timing']:\n",
        "            train_t = metrics['timing']['train_total_s']\n",
        "            print(f\"\\nüéì TRAINING ({train_t:.2f}s total, {train_t/total_runtime*100:.1f}% of runtime):\")\n",
        "            avg_epoch = train_t / metrics['train'].get('best_epoch', 1) if 'train' in metrics else 0\n",
        "            print(f\"  ‚Ä¢ Avg Epoch Time:    {avg_epoch:6.2f}s\")\n",
        "            print(f\"  ‚Ä¢ Total Epochs:      {metrics['train'].get('best_epoch', 0)}\")\n",
        "\n",
        "            # Show detailed breakdown from epoch 1\n",
        "            if 'epoch_details' in metrics['timing'] and 'epoch_1' in metrics['timing']['epoch_details']:\n",
        "                ep1 = metrics['timing']['epoch_details']['epoch_1']\n",
        "                train_detail = ep1.get('train', {})\n",
        "                print(f\"\\n  Epoch Breakdown (Epoch 1):\")\n",
        "                print(f\"    - Forward Pass:    {train_detail.get('forward_pass', 0):6.3f}s ({train_detail.get('forward_pass', 0)/ep1['total']*100:5.1f}%)\")\n",
        "                print(f\"    - Backward Pass:   {train_detail.get('backward_pass', 0):6.3f}s ({train_detail.get('backward_pass', 0)/ep1['total']*100:5.1f}%)\")\n",
        "                print(f\"    - Optimizer Step:  {train_detail.get('optimizer_step', 0):6.3f}s ({train_detail.get('optimizer_step', 0)/ep1['total']*100:5.1f}%)\")\n",
        "                print(f\"    - Loss Compute:    {train_detail.get('loss_computation', 0):6.3f}s ({train_detail.get('loss_computation', 0)/ep1['total']*100:5.1f}%)\")\n",
        "                print(f\"    - Val Eval:        {ep1.get('val', {}).get('forward_pass', 0):6.3f}s ({ep1.get('val', {}).get('forward_pass', 0)/ep1['total']*100:5.1f}%)\")\n",
        "                print(f\"    - Test Eval:       {ep1.get('test', {}).get('forward_pass', 0):6.3f}s ({ep1.get('test', {}).get('forward_pass', 0)/ep1['total']*100:5.1f}%)\")\n",
        "\n",
        "        # High-level summary\n",
        "        print(f\"\\nüìà HIGH-LEVEL SUMMARY:\")\n",
        "        data_pct = metrics['timing'].get('data_load', {}).get('total', 0) / total_runtime * 100\n",
        "        graph_pct = metrics['timing'].get('graph_build', {}).get('total', 0) / total_runtime * 100\n",
        "        train_pct = metrics['timing'].get('train_total_s', 0) / total_runtime * 100\n",
        "\n",
        "        print(f\"  ‚Ä¢ Data Loading:      {data_pct:5.1f}% of total runtime\")\n",
        "        print(f\"  ‚Ä¢ Graph Building:    {graph_pct:5.1f}% of total runtime\")\n",
        "        print(f\"  ‚Ä¢ Training:          {train_pct:5.1f}% of total runtime\")\n",
        "        print(f\"  ‚Ä¢ Total Runtime:     {total_runtime:.2f}s\")\n",
        "\n",
        "        # GPU summary if available\n",
        "        if torch.cuda.is_available() and 'gpu_memory' in metrics:\n",
        "            peak_gb = metrics['gpu_memory'].get('peak_allocated', 0) / 1024**3\n",
        "            print(f\"  ‚Ä¢ Peak GPU Memory:   {peak_gb:.2f} GB\")\n",
        "\n",
        "        # Identify bottlenecks\n",
        "        print(f\"\\nüîç BOTTLENECK ANALYSIS:\")\n",
        "        bottlenecks = []\n",
        "\n",
        "        if 'data_load' in metrics['timing']:\n",
        "            data_t = metrics['timing']['data_load']\n",
        "            for key, val in data_t.items():\n",
        "                if key != 'total' and val > 1.0:  # More than 1 second\n",
        "                    bottlenecks.append((f\"Data: {key}\", val, val/total_runtime*100))\n",
        "\n",
        "        if 'graph_build' in metrics['timing']:\n",
        "            graph_t = metrics['timing']['graph_build']\n",
        "            for key, val in graph_t.items():\n",
        "                if key != 'total' and val > 1.0:\n",
        "                    bottlenecks.append((f\"Graph: {key}\", val, val/total_runtime*100))\n",
        "\n",
        "        if bottlenecks:\n",
        "            bottlenecks.sort(key=lambda x: x[1], reverse=True)\n",
        "            for i, (name, time_s, pct) in enumerate(bottlenecks[:5], 1):\n",
        "                print(f\"  {i}. {name:30s} {time_s:6.2f}s ({pct:5.1f}%)\")\n",
        "        else:\n",
        "            print(\"  No major bottlenecks detected (all operations < 1s)\")\n",
        "\n",
        "        out_path = os.path.join(RESULTS_PATH, f\"metrics_gpt5_v2_{run_id}.json\")\n",
        "        save_json(metrics, out_path)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"UHG IDS Model GPT5 v2 - Training Complete\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"Results saved to: {out_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxNUk5etl4cd",
        "outputId": "7c7b675e-6ea2-410d-f17f-4ef0cd7a9963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "\n",
            "================================================================================\n",
            "üñ•Ô∏è  HARDWARE CONFIGURATION\n",
            "================================================================================\n",
            "‚úÖ GPU Detected:\n",
            "   ‚Ä¢ Model: NVIDIA L4\n",
            "   ‚Ä¢ Memory: 22.2 GB\n",
            "   ‚Ä¢ CUDA Version: 12.4\n",
            "   ‚Ä¢ Compute Capability: 8.9\n",
            "   ‚Ä¢ Device: cuda:0\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Loading data from: /content/drive/MyDrive/CIC_data.csv\n",
            "  ‚è±Ô∏è  CSV read: 35.43s\n",
            "\n",
            "Unique labels in the dataset: ['BENIGN' 'DDoS' 'PortScan' 'Bot' 'Infiltration'\n",
            " 'Web Attack ÔøΩ Brute Force' 'Web Attack ÔøΩ XSS'\n",
            " 'Web Attack ÔøΩ Sql Injection' 'FTP-Patator' 'SSH-Patator' 'DoS slowloris'\n",
            " 'DoS Slowhttptest' 'DoS Hulk' 'DoS GoldenEye' 'Heartbleed']\n",
            "\n",
            "Label distribution in the dataset:\n",
            "Label\n",
            "BENIGN                        2273097\n",
            "DoS Hulk                       231073\n",
            "PortScan                       158930\n",
            "DDoS                           128027\n",
            "DoS GoldenEye                   10293\n",
            "FTP-Patator                      7938\n",
            "SSH-Patator                      5897\n",
            "DoS slowloris                    5796\n",
            "DoS Slowhttptest                 5499\n",
            "Bot                              1966\n",
            "Web Attack ÔøΩ Brute Force         1507\n",
            "Web Attack ÔøΩ XSS                  652\n",
            "Infiltration                       36\n",
            "Web Attack ÔøΩ Sql Injection         21\n",
            "Heartbleed                         11\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Applying random sampling (frac=0.1)...\n",
            "  ‚è±Ô∏è  Sampling: 0.50s\n",
            "\n",
            "Sampled label distribution:\n",
            "Label\n",
            "BENIGN                        227293\n",
            "DoS Hulk                       23097\n",
            "PortScan                       15910\n",
            "DDoS                           12851\n",
            "DoS GoldenEye                   1024\n",
            "FTP-Patator                      825\n",
            "DoS slowloris                    580\n",
            "SSH-Patator                      571\n",
            "DoS Slowhttptest                 525\n",
            "Bot                              194\n",
            "Web Attack ÔøΩ Brute Force         141\n",
            "Web Attack ÔøΩ XSS                  56\n",
            "Infiltration                       3\n",
            "Heartbleed                         2\n",
            "Web Attack ÔøΩ Sql Injection         2\n",
            "Name: count, dtype: int64\n",
            "  ‚è±Ô∏è  Convert to numeric: 0.39s\n",
            "  ‚è±Ô∏è  Fill NaN/inf: 0.84s\n",
            "  ‚è±Ô∏è  Scaling: 0.37s\n",
            "  ‚è±Ô∏è  Convert to tensors: 0.02s\n",
            "\n",
            "Preprocessing complete.\n",
            "Feature shape: torch.Size([283074, 77])\n",
            "Number of unique labels: 15\n",
            "\n",
            "‚è±Ô∏è  Total data loading time: 37.95s\n",
            "\n",
            "Creating graph structure...\n",
            "\n",
            "Computing KNN graph with k=2...\n",
            "  ‚Ä¢ Input shape: (283074, 77)\n",
            "  ‚Ä¢ Number of samples: 283,074\n",
            "  ‚Ä¢ Number of features: 77\n",
            "  ‚Ä¢ Using n_jobs=-1 (all CPU cores)\n",
            "  ‚Ä¢ No PCA (77 features - vs v4.5 with 20 features)\n",
            "  ‚Ä¢ Expected: Slower KNN than v4.5 due to higher dimensionality\n",
            "  ‚úÖ KNN computation: 102.24s\n",
            "  ‚è±Ô∏è  Edge index creation: 0.13s\n",
            "Edge index shape: torch.Size([2, 566148])\n",
            "  ‚è±Ô∏è  UHG projection: 1.57s\n",
            "Feature shape with homogeneous coordinate: torch.Size([283074, 78])\n",
            "UHG Constraint Check (initial features):\n",
            "  Max violation: 0.062500\n",
            "  Mean violation: 0.000003\n",
            "  ‚ö†Ô∏è WARNING: Constraints violated!\n",
            "\n",
            "Train size: 198151, Val size: 42461, Test size: 42462\n",
            "\n",
            "‚è±Ô∏è  Total graph construction time: 105.49s\n",
            "\n",
            "‚úÖ Using standard CrossEntropyLoss (no class weighting)\n",
            "\n",
            "üíæ GPU Memory (before training):\n",
            "   ‚Ä¢ Allocated: 0.09 GB\n",
            "   ‚Ä¢ Reserved:  0.27 GB\n",
            "\n",
            "Starting training...\n",
            "\n",
            "‚è±Ô∏è  Epoch 1 Detailed Timing:\n",
            "    Train: Forward=1.607s, Backward=1.585s, Optimizer=0.658s\n",
            "    Val:   Forward=0.042s\n",
            "    Test:  Forward=0.042s\n",
            "Epoch 001 | Loss 3.0845 | Val 0.6264 | Test 0.6253 | LR 0.01000 | 5.47s | (saved)\n",
            "\n",
            "‚è±Ô∏è  Epoch 2 Detailed Timing:\n",
            "    Train: Forward=0.043s, Backward=0.008s, Optimizer=0.001s\n",
            "    Val:   Forward=0.042s\n",
            "    Test:  Forward=0.042s\n",
            "Epoch 002 | Loss 1.7592 | Val 0.8219 | Test 0.8218 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 003 | Loss 0.9829 | Val 0.8595 | Test 0.8591 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 004 | Loss 0.6923 | Val 0.8753 | Test 0.8737 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 005 | Loss 0.5513 | Val 0.8996 | Test 0.8981 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 006 | Loss 0.4774 | Val 0.9046 | Test 0.9053 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 007 | Loss 0.4326 | Val 0.9089 | Test 0.9096 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 008 | Loss 0.3866 | Val 0.9318 | Test 0.9308 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 009 | Loss 0.3428 | Val 0.9373 | Test 0.9360 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 010 | Loss 0.3086 | Val 0.9332 | Test 0.9305 | LR 0.01000 | 0.21s\n",
            "Epoch 013 | Loss 0.2751 | Val 0.9411 | Test 0.9395 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 014 | Loss 0.2686 | Val 0.9438 | Test 0.9436 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 016 | Loss 0.2466 | Val 0.9468 | Test 0.9461 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 018 | Loss 0.2318 | Val 0.9470 | Test 0.9465 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 019 | Loss 0.2237 | Val 0.9477 | Test 0.9470 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 020 | Loss 0.2167 | Val 0.9478 | Test 0.9468 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 021 | Loss 0.2082 | Val 0.9490 | Test 0.9481 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 022 | Loss 0.1994 | Val 0.9505 | Test 0.9496 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 023 | Loss 0.1927 | Val 0.9538 | Test 0.9526 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 026 | Loss 0.1784 | Val 0.9547 | Test 0.9545 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 028 | Loss 0.1704 | Val 0.9549 | Test 0.9552 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 030 | Loss 0.1618 | Val 0.9540 | Test 0.9541 | LR 0.01000 | 0.21s\n",
            "Epoch 031 | Loss 0.1604 | Val 0.9561 | Test 0.9559 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 032 | Loss 0.1546 | Val 0.9576 | Test 0.9578 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 033 | Loss 0.1518 | Val 0.9584 | Test 0.9586 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 034 | Loss 0.1489 | Val 0.9585 | Test 0.9588 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 035 | Loss 0.1469 | Val 0.9594 | Test 0.9592 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 036 | Loss 0.1452 | Val 0.9597 | Test 0.9598 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 037 | Loss 0.1413 | Val 0.9599 | Test 0.9598 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 040 | Loss 0.1355 | Val 0.9602 | Test 0.9606 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 041 | Loss 0.1338 | Val 0.9604 | Test 0.9607 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 042 | Loss 0.1304 | Val 0.9606 | Test 0.9610 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 043 | Loss 0.1296 | Val 0.9610 | Test 0.9612 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 044 | Loss 0.1264 | Val 0.9616 | Test 0.9617 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 045 | Loss 0.1250 | Val 0.9624 | Test 0.9627 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 046 | Loss 0.1235 | Val 0.9627 | Test 0.9632 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 047 | Loss 0.1226 | Val 0.9631 | Test 0.9633 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 048 | Loss 0.1212 | Val 0.9632 | Test 0.9632 | LR 0.01000 | 0.21s | (saved)\n",
            "\n",
            "‚è±Ô∏è  Epoch 50 Detailed Timing:\n",
            "    Train: Forward=0.043s, Backward=0.007s, Optimizer=0.001s\n",
            "    Val:   Forward=0.042s\n",
            "    Test:  Forward=0.042s\n",
            "Epoch 050 | Loss 0.1185 | Val 0.9631 | Test 0.9628 | LR 0.01000 | 0.21s\n",
            "Epoch 051 | Loss 0.1171 | Val 0.9632 | Test 0.9628 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 053 | Loss 0.1151 | Val 0.9636 | Test 0.9630 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 054 | Loss 0.1142 | Val 0.9642 | Test 0.9638 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 055 | Loss 0.1114 | Val 0.9647 | Test 0.9645 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 056 | Loss 0.1116 | Val 0.9650 | Test 0.9647 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 057 | Loss 0.1100 | Val 0.9650 | Test 0.9647 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 058 | Loss 0.1096 | Val 0.9650 | Test 0.9647 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 060 | Loss 0.1076 | Val 0.9658 | Test 0.9656 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 061 | Loss 0.1067 | Val 0.9659 | Test 0.9655 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 062 | Loss 0.1052 | Val 0.9660 | Test 0.9657 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 063 | Loss 0.1055 | Val 0.9666 | Test 0.9663 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 064 | Loss 0.1046 | Val 0.9670 | Test 0.9666 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 065 | Loss 0.1036 | Val 0.9674 | Test 0.9669 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 069 | Loss 0.1008 | Val 0.9675 | Test 0.9669 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 070 | Loss 0.0999 | Val 0.9677 | Test 0.9670 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 071 | Loss 0.0998 | Val 0.9679 | Test 0.9673 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 072 | Loss 0.0999 | Val 0.9682 | Test 0.9675 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 075 | Loss 0.0970 | Val 0.9683 | Test 0.9676 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 076 | Loss 0.0970 | Val 0.9684 | Test 0.9676 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 079 | Loss 0.0955 | Val 0.9685 | Test 0.9678 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 080 | Loss 0.0948 | Val 0.9685 | Test 0.9679 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 081 | Loss 0.0944 | Val 0.9687 | Test 0.9680 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 082 | Loss 0.0931 | Val 0.9688 | Test 0.9681 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 083 | Loss 0.0927 | Val 0.9688 | Test 0.9681 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 084 | Loss 0.0931 | Val 0.9689 | Test 0.9681 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 088 | Loss 0.0912 | Val 0.9689 | Test 0.9681 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 089 | Loss 0.0908 | Val 0.9690 | Test 0.9683 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 090 | Loss 0.0900 | Val 0.9691 | Test 0.9683 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 091 | Loss 0.0893 | Val 0.9691 | Test 0.9685 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 092 | Loss 0.0896 | Val 0.9692 | Test 0.9685 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 093 | Loss 0.0896 | Val 0.9692 | Test 0.9685 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 094 | Loss 0.0888 | Val 0.9693 | Test 0.9685 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 095 | Loss 0.0885 | Val 0.9693 | Test 0.9686 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 096 | Loss 0.0883 | Val 0.9694 | Test 0.9686 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 097 | Loss 0.0876 | Val 0.9697 | Test 0.9688 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 098 | Loss 0.0874 | Val 0.9699 | Test 0.9691 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 099 | Loss 0.0873 | Val 0.9701 | Test 0.9693 | LR 0.01000 | 0.21s | (saved)\n",
            "\n",
            "‚è±Ô∏è  Epoch 100 Detailed Timing:\n",
            "    Train: Forward=0.043s, Backward=0.007s, Optimizer=0.001s\n",
            "    Val:   Forward=0.042s\n",
            "    Test:  Forward=0.042s\n",
            "Epoch 100 | Loss 0.0865 | Val 0.9701 | Test 0.9694 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 101 | Loss 0.0866 | Val 0.9702 | Test 0.9695 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 102 | Loss 0.0862 | Val 0.9703 | Test 0.9695 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 103 | Loss 0.0854 | Val 0.9703 | Test 0.9696 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 104 | Loss 0.0848 | Val 0.9704 | Test 0.9697 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 105 | Loss 0.0849 | Val 0.9705 | Test 0.9698 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 106 | Loss 0.0849 | Val 0.9706 | Test 0.9699 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 107 | Loss 0.0842 | Val 0.9707 | Test 0.9700 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 108 | Loss 0.0839 | Val 0.9707 | Test 0.9701 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 110 | Loss 0.0835 | Val 0.9707 | Test 0.9701 | LR 0.01000 | 0.21s\n",
            "Epoch 113 | Loss 0.0824 | Val 0.9708 | Test 0.9701 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 114 | Loss 0.0818 | Val 0.9709 | Test 0.9702 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 115 | Loss 0.0818 | Val 0.9710 | Test 0.9702 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 116 | Loss 0.0821 | Val 0.9712 | Test 0.9704 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 117 | Loss 0.0819 | Val 0.9712 | Test 0.9705 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 118 | Loss 0.0808 | Val 0.9713 | Test 0.9705 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 120 | Loss 0.0807 | Val 0.9713 | Test 0.9706 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 121 | Loss 0.0802 | Val 0.9715 | Test 0.9707 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 123 | Loss 0.0799 | Val 0.9716 | Test 0.9709 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 124 | Loss 0.0797 | Val 0.9716 | Test 0.9709 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 125 | Loss 0.0798 | Val 0.9716 | Test 0.9710 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 130 | Loss 0.0789 | Val 0.9717 | Test 0.9711 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 131 | Loss 0.0784 | Val 0.9718 | Test 0.9711 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 133 | Loss 0.0777 | Val 0.9718 | Test 0.9711 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 134 | Loss 0.0773 | Val 0.9719 | Test 0.9711 | LR 0.01000 | 0.22s | (saved)\n",
            "Epoch 138 | Loss 0.0763 | Val 0.9719 | Test 0.9712 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 139 | Loss 0.0761 | Val 0.9720 | Test 0.9713 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 140 | Loss 0.0758 | Val 0.9720 | Test 0.9714 | LR 0.01000 | 0.21s | (saved)\n",
            "Epoch 147 | Loss 0.0753 | Val 0.9721 | Test 0.9713 | LR 0.00500 | 0.21s | (saved)\n",
            "Epoch 148 | Loss 0.0753 | Val 0.9721 | Test 0.9714 | LR 0.00500 | 0.21s | (saved)\n",
            "Epoch 149 | Loss 0.0748 | Val 0.9722 | Test 0.9714 | LR 0.00500 | 0.21s | (saved)\n",
            "Epoch 150 | Loss 0.0746 | Val 0.9724 | Test 0.9715 | LR 0.00500 | 0.21s | (saved)\n",
            "Epoch 158 | Loss 0.0744 | Val 0.9724 | Test 0.9716 | LR 0.00250 | 0.21s | (saved)\n",
            "Epoch 160 | Loss 0.0742 | Val 0.9724 | Test 0.9716 | LR 0.00250 | 0.21s\n",
            "Epoch 161 | Loss 0.0742 | Val 0.9724 | Test 0.9716 | LR 0.00250 | 0.21s | (saved)\n",
            "Epoch 166 | Loss 0.0734 | Val 0.9724 | Test 0.9716 | LR 0.00125 | 0.21s | (saved)\n",
            "Epoch 167 | Loss 0.0740 | Val 0.9725 | Test 0.9716 | LR 0.00125 | 0.21s | (saved)\n",
            "Epoch 169 | Loss 0.0740 | Val 0.9725 | Test 0.9716 | LR 0.00125 | 0.21s | (saved)\n",
            "Epoch 170 | Loss 0.0735 | Val 0.9725 | Test 0.9716 | LR 0.00125 | 0.21s\n",
            "Epoch 180 | Loss 0.0736 | Val 0.9725 | Test 0.9716 | LR 0.00031 | 0.21s\n",
            "Early stopping.\n",
            "\n",
            "‚è±Ô∏è  Average epoch time: 0.25s\n",
            "\n",
            "üíæ GPU Memory Usage Summary:\n",
            "   ‚Ä¢ Peak Allocated: 1.56 GB\n",
            "   ‚Ä¢ Final Allocated: 0.11 GB\n",
            "   ‚Ä¢ Final Reserved: 1.87 GB\n",
            "\n",
            "Loading best model for final evaluation...\n",
            "\n",
            "================================================================================\n",
            "POST-TRAINING UHG CONSTRAINT VERIFICATION\n",
            "================================================================================\n",
            "UHG Constraint Check (after layer 1):\n",
            "  Max violation: 3.000000\n",
            "  Mean violation: 0.000039\n",
            "  ‚ö†Ô∏è WARNING: Constraints violated!\n",
            "\n",
            "================================================================================\n",
            "Test Set - Detailed Performance Report\n",
            "================================================================================\n",
            "\n",
            "‚ö†Ô∏è  WARNING: 2 classes not present in test set:\n",
            "  ‚Ä¢ Infiltration\n",
            "  ‚Ä¢ Web Attack ÔøΩ Sql Injection\n",
            "  (This is normal with small sample sizes and rare classes)\n",
            "\n",
            "Overall Accuracy: 0.9716\n",
            "Classes evaluated: 13/15\n",
            "\n",
            "Per-Class Classification Report:\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "                  BENIGN     0.9838    0.9811    0.9824     34017\n",
            "                    DDoS     0.9937    0.9688    0.9811      1953\n",
            "                PortScan     0.8110    0.9909    0.8920      2430\n",
            "                     Bot     0.0000    0.0000    0.0000        25\n",
            "Web Attack ÔøΩ Brute Force     0.0000    0.0000    0.0000        21\n",
            "        Web Attack ÔøΩ XSS     0.0000    0.0000    0.0000         5\n",
            "             FTP-Patator     1.0000    0.4252    0.5967       127\n",
            "             SSH-Patator     1.0000    0.4588    0.6290        85\n",
            "           DoS slowloris     0.9710    0.8171    0.8874        82\n",
            "        DoS Slowhttptest     0.8642    0.8750    0.8696        80\n",
            "                DoS Hulk     0.9788    0.9220    0.9496      3463\n",
            "           DoS GoldenEye     0.9875    0.9133    0.9489       173\n",
            "              Heartbleed     0.0000    0.0000    0.0000         1\n",
            "\n",
            "                accuracy                         0.9716     42462\n",
            "               macro avg     0.6608    0.5656    0.5951     42462\n",
            "            weighted avg     0.9726    0.9716    0.9709     42462\n",
            "\n",
            "\n",
            "Per-Class Accuracy:\n",
            "  BENIGN                        : 0.9811 (34017 samples)\n",
            "  DDoS                          : 0.9688 (1953 samples)\n",
            "  PortScan                      : 0.9909 (2430 samples)\n",
            "  Bot                           : 0.0000 (25 samples)\n",
            "  Web Attack ÔøΩ Brute Force      : 0.0000 (21 samples)\n",
            "  Web Attack ÔøΩ XSS              : 0.0000 (5 samples)\n",
            "  FTP-Patator                   : 0.4252 (127 samples)\n",
            "  SSH-Patator                   : 0.4588 (85 samples)\n",
            "  DoS slowloris                 : 0.8171 (82 samples)\n",
            "  DoS Slowhttptest              : 0.8750 (80 samples)\n",
            "  DoS Hulk                      : 0.9220 (3463 samples)\n",
            "  DoS GoldenEye                 : 0.9133 (173 samples)\n",
            "  Heartbleed                    : 0.0000 (1 samples)\n",
            "\n",
            "F1 Score (Macro):    0.5951\n",
            "F1 Score (Weighted): 0.9709\n",
            "\n",
            "Final Test Accuracy: 0.9716\n",
            "\n",
            "================================================================================\n",
            "‚è±Ô∏è  COMPREHENSIVE TIMING BREAKDOWN\n",
            "================================================================================\n",
            "\n",
            "üìä DATA LOADING (37.95s total):\n",
            "  ‚Ä¢ CSV Read:           35.43s ( 18.6%)\n",
            "  ‚Ä¢ Sampling:            0.50s (  0.3%)\n",
            "  ‚Ä¢ To Numeric:          0.39s (  0.2%)\n",
            "  ‚Ä¢ Fill NaN/Inf:        0.84s (  0.4%)\n",
            "  ‚Ä¢ Scaling:             0.37s (  0.2%)\n",
            "  ‚Ä¢ To Tensors:          0.02s (  0.0%)\n",
            "\n",
            "üï∏Ô∏è  GRAPH CONSTRUCTION (105.49s total):\n",
            "  ‚Ä¢ KNN Computation:   102.24s ( 53.7%)\n",
            "  ‚Ä¢ Edge Index:          0.13s (  0.1%)\n",
            "  ‚Ä¢ UHG Projection:      1.57s (  0.8%)\n",
            "  ‚Ä¢ Constraint Check:    0.85s (  0.4%)\n",
            "\n",
            "üéì TRAINING (46.41s total, 24.4% of runtime):\n",
            "  ‚Ä¢ Avg Epoch Time:      0.27s\n",
            "  ‚Ä¢ Total Epochs:      169\n",
            "\n",
            "  Epoch Breakdown (Epoch 1):\n",
            "    - Forward Pass:     1.607s ( 29.4%)\n",
            "    - Backward Pass:    1.585s ( 29.0%)\n",
            "    - Optimizer Step:   0.658s ( 12.0%)\n",
            "    - Loss Compute:     0.428s (  7.8%)\n",
            "    - Val Eval:         0.042s (  0.8%)\n",
            "    - Test Eval:        0.042s (  0.8%)\n",
            "\n",
            "üìà HIGH-LEVEL SUMMARY:\n",
            "  ‚Ä¢ Data Loading:       19.9% of total runtime\n",
            "  ‚Ä¢ Graph Building:     55.4% of total runtime\n",
            "  ‚Ä¢ Training:           24.4% of total runtime\n",
            "  ‚Ä¢ Total Runtime:     190.49s\n",
            "  ‚Ä¢ Peak GPU Memory:   1.56 GB\n",
            "\n",
            "üîç BOTTLENECK ANALYSIS:\n",
            "  1. Graph: knn_computation         102.24s ( 53.7%)\n",
            "  2. Data: csv_read                  35.43s ( 18.6%)\n",
            "  3. Graph: projective_normalize      1.57s (  0.8%)\n",
            "Saved metrics to: /content/drive/MyDrive/uhg_ids_results/metrics_gpt5_v2_20251003T232240Z.json\n",
            "\n",
            "================================================================================\n",
            "UHG IDS Model GPT5 v2 - Training Complete\n",
            "================================================================================\n",
            "Results saved to: /content/drive/MyDrive/uhg_ids_results/metrics_gpt5_v2_20251003T232240Z.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vY8d51gpmQ8_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}