{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwAFv3asjN1S",
        "outputId": "94117d49-7c43-4cf3-af3d-561e2514e5a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl.metadata (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting geoopt\n",
            "  Downloading geoopt-0.5.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.13.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.10.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading geoopt-0.5.0-py3-none-any.whl (90 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.1/90.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric, geoopt\n",
            "Successfully installed geoopt-0.5.0 torch-geometric-2.5.3\n",
            "Mounted at /content/drive\n",
            "Using device: cuda\n",
            "Unique labels in the dataset: ['BENIGN' 'DDoS' 'PortScan' 'Bot' 'Infiltration'\n",
            " 'Web Attack � Brute Force' 'Web Attack � XSS'\n",
            " 'Web Attack � Sql Injection' 'FTP-Patator' 'SSH-Patator' 'DoS slowloris'\n",
            " 'DoS Slowhttptest' 'DoS Hulk' 'DoS GoldenEye' 'Heartbleed']\n",
            "Label distribution in the dataset:\n",
            "Label\n",
            "BENIGN                        2273097\n",
            "DoS Hulk                       231073\n",
            "PortScan                       158930\n",
            "DDoS                           128027\n",
            "DoS GoldenEye                   10293\n",
            "FTP-Patator                      7938\n",
            "SSH-Patator                      5897\n",
            "DoS slowloris                    5796\n",
            "DoS Slowhttptest                 5499\n",
            "Bot                              1966\n",
            "Web Attack � Brute Force         1507\n",
            "Web Attack � XSS                  652\n",
            "Infiltration                       36\n",
            "Web Attack � Sql Injection         21\n",
            "Heartbleed                         11\n",
            "Name: count, dtype: int64\n",
            "Input tensor shape: torch.Size([283074, 77])\n",
            "Node features transformed to UHG space: torch.Size([283074, 78])\n",
            "Edge index shape: torch.Size([2, 566148])\n",
            "Train size: 198151, Val size: 42461, Test size: 42462\n",
            "Using learning rate: 0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 12385/12385 [00:53<00:00, 232.87it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 247.69it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 245.92it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:51<00:00, 242.52it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 246.98it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 244.68it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 245.12it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 242.89it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 245.76it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 248.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10, Loss: 0.0285, Val Accuracy: 0.9507, Test Accuracy: 0.9517, Learning Rate: 0.010000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 244.17it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 246.54it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 242.86it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 247.83it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 245.50it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 246.15it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 247.06it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 244.53it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 247.61it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:51<00:00, 242.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 20, Loss: 0.0280, Val Accuracy: 0.9534, Test Accuracy: 0.9555, Learning Rate: 0.010000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 246.58it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 246.10it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 245.31it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 246.98it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:51<00:00, 242.55it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 248.92it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 244.69it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 246.39it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 246.32it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 244.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 30, Loss: 0.0201, Val Accuracy: 0.9612, Test Accuracy: 0.9624, Learning Rate: 0.005000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 248.13it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 244.03it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 246.93it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 243.71it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 250.01it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 246.97it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 242.97it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 247.83it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 245.76it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 248.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 40, Loss: 0.0192, Val Accuracy: 0.9670, Test Accuracy: 0.9676, Learning Rate: 0.005000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 12385/12385 [00:51<00:00, 240.98it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 248.31it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 247.43it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 244.53it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 247.24it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 245.22it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 250.09it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 245.04it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 246.90it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 248.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 50, Loss: 0.0172, Val Accuracy: 0.9704, Test Accuracy: 0.9719, Learning Rate: 0.002500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 247.26it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 248.44it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 245.15it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 247.92it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 249.77it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 243.82it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 247.46it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 246.66it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 250.66it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 245.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 60, Loss: 0.0162, Val Accuracy: 0.9721, Test Accuracy: 0.9715, Learning Rate: 0.002500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 247.30it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 248.62it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 246.59it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 248.33it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 245.05it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 249.07it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 245.73it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 245.01it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:51<00:00, 242.36it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 247.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 70, Loss: 0.0154, Val Accuracy: 0.9689, Test Accuracy: 0.9704, Learning Rate: 0.001250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 249.73it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 243.14it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 246.45it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 244.11it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 249.72it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 242.93it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 246.56it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 248.72it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 244.43it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 246.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 80, Loss: 0.0145, Val Accuracy: 0.9743, Test Accuracy: 0.9761, Learning Rate: 0.001250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 243.62it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 247.87it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 243.73it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 247.22it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 246.06it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 245.69it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 247.55it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 243.96it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 246.57it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 243.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 90, Loss: 0.0142, Val Accuracy: 0.9776, Test Accuracy: 0.9777, Learning Rate: 0.000625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 246.57it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 247.13it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 245.24it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 249.65it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 244.96it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 247.06it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:51<00:00, 240.48it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 249.13it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 246.19it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 246.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 100, Loss: 0.0132, Val Accuracy: 0.9789, Test Accuracy: 0.9792, Learning Rate: 0.000625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 247.39it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 246.04it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 247.58it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:51<00:00, 240.37it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 247.81it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 246.31it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 247.54it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 247.05it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:51<00:00, 241.97it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 248.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 110, Loss: 0.0125, Val Accuracy: 0.9813, Test Accuracy: 0.9820, Learning Rate: 0.000313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 243.97it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 246.22it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 244.73it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 249.28it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 246.16it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:51<00:00, 240.31it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 248.08it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 245.74it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 246.51it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:51<00:00, 242.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 120, Loss: 0.0128, Val Accuracy: 0.9812, Test Accuracy: 0.9815, Learning Rate: 0.000313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 248.31it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 247.03it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 244.92it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 247.43it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 246.12it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 248.07it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:51<00:00, 240.72it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 248.22it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 247.35it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 247.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 130, Loss: 0.0119, Val Accuracy: 0.9817, Test Accuracy: 0.9823, Learning Rate: 0.000156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 247.43it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 243.85it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:49<00:00, 249.34it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:51<00:00, 241.32it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 247.28it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:52<00:00, 237.47it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 247.37it/s]\n",
            "Training: 100%|██████████| 12385/12385 [00:50<00:00, 245.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping\n",
            "Final Learning Rate: 0.000156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-6867adebd2f9>:332: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(best_model_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test Accuracy: 0.9836\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install torch torch-geometric geoopt tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "from scipy.sparse import coo_matrix\n",
        "from tqdm import tqdm\n",
        "import geoopt\n",
        "from geoopt import PoincareBall\n",
        "from torch.utils.data import DataLoader\n",
        "from torch_geometric.data import Data\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Check for CUDA availability and set the device accordingly. One of the two options below must be commented out\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#device = 'cpu'\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/drive/MyDrive/CIC_data.csv'  # Update this with the actual path to your CSV file in Google Drive\n",
        "data = pd.read_csv(file_path, low_memory=False)\n",
        "\n",
        "# Data preprocessing steps\n",
        "data.columns = data.columns.str.strip()\n",
        "data['Label'] = data['Label'].str.strip()\n",
        "\n",
        "# Verify unique labels and their distribution\n",
        "unique_labels = data['Label'].unique()\n",
        "print(f\"Unique labels in the dataset: {unique_labels}\")\n",
        "label_counts = data['Label'].value_counts()\n",
        "print(\"Label distribution in the dataset:\")\n",
        "print(label_counts)\n",
        "\n",
        "# Sample a smaller fraction of the rows (e.g., 20%)\n",
        "data_sampled = data.sample(frac=0.10, random_state=42)\n",
        "\n",
        "# Convert all columns to numeric, coerce errors to NaN\n",
        "data_numeric = data_sampled.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Fill NaN values with the mean of each column\n",
        "data_filled = data_numeric.fillna(data_numeric.mean())\n",
        "\n",
        "# Handle infinite and very large values\n",
        "data_filled = data_filled.replace([np.inf, -np.inf], np.nan)\n",
        "data_filled = data_filled.fillna(data_filled.max())\n",
        "\n",
        "# Check again for any remaining NaNs and fill them\n",
        "if data_filled.isnull().values.any():\n",
        "    data_filled = data_filled.fillna(0)\n",
        "\n",
        "# Extract labels\n",
        "labels = data_sampled['Label']\n",
        "data_filled = data_filled.drop(columns=['Label'])\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data_filled)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "node_features = torch.tensor(data_scaled, dtype=torch.float32).to(device)\n",
        "print(f\"Input tensor shape: {node_features.shape}\")\n",
        "\n",
        "# UHG Operations\n",
        "def uhg_quadrance(a, b, eps=1e-9):\n",
        "    \"\"\"Compute UHG quadrance between two points.\"\"\"\n",
        "    dot_product = torch.sum(a * b, dim=-1)\n",
        "    return 1 - (dot_product ** 2) / ((torch.sum(a ** 2, dim=-1) - a[:, -1] ** 2 + eps) * (torch.sum(b ** 2, dim=-1) - b[:, -1] ** 2 + eps))\n",
        "\n",
        "def uhg_spread(L, M, eps=1e-9):\n",
        "    \"\"\"Compute UHG spread between two lines.\"\"\"\n",
        "    dot_product = torch.sum(L * M, dim=-1)\n",
        "    return 1 - (dot_product ** 2) / ((torch.sum(L ** 2, dim=-1) - L[:, -1] ** 2 + eps) * (torch.sum(M ** 2, dim=-1) - M[:, -1] ** 2 + eps))\n",
        "\n",
        "# Transform the node features to UHG space\n",
        "def to_uhg_space(x):\n",
        "    \"\"\"Transform Euclidean coordinates to UHG space.\"\"\"\n",
        "    return torch.cat([x, torch.ones(x.shape[0], 1, device=x.device)], dim=-1)\n",
        "\n",
        "\n",
        "node_features_uhg = to_uhg_space(node_features)\n",
        "print(f\"Node features transformed to UHG space: {node_features_uhg.shape}\")\n",
        "\n",
        "# Create a k-nearest neighbors graph\n",
        "k = 2  # Set k value to 2\n",
        "knn_graph = kneighbors_graph(data_scaled, k, mode='connectivity', include_self=False)\n",
        "\n",
        "# Convert knn_graph to COO format\n",
        "knn_graph_coo = coo_matrix(knn_graph)\n",
        "\n",
        "# Create edge index\n",
        "edge_index_np = np.array([knn_graph_coo.row, knn_graph_coo.col])\n",
        "edge_index = torch.from_numpy(edge_index_np).long().to(device)\n",
        "print(f\"Edge index shape: {edge_index.shape}\")\n",
        "\n",
        "# Convert labels to numeric\n",
        "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "labels_numeric = labels.map(label_mapping).values\n",
        "labels_tensor = torch.tensor(labels_numeric, dtype=torch.long).to(device)\n",
        "\n",
        "# Create 70/15/15 train/val/test split\n",
        "total_samples = node_features_uhg.size(0)\n",
        "train_size = int(0.7 * total_samples)\n",
        "val_size = int(0.15 * total_samples)\n",
        "\n",
        "indices = torch.randperm(total_samples)\n",
        "train_indices = indices[:train_size]\n",
        "val_indices = indices[train_size:train_size+val_size]\n",
        "test_indices = indices[train_size+val_size:]\n",
        "\n",
        "train_mask = torch.zeros(total_samples, dtype=torch.bool)\n",
        "val_mask = torch.zeros(total_samples, dtype=torch.bool)\n",
        "test_mask = torch.zeros(total_samples, dtype=torch.bool)\n",
        "\n",
        "train_mask[train_indices] = True\n",
        "val_mask[val_indices] = True\n",
        "test_mask[test_indices] = True\n",
        "\n",
        "# Create the PyTorch Geometric data object\n",
        "graph_data = Data(x=node_features_uhg, edge_index=edge_index, y=labels_tensor,\n",
        "                  train_mask=train_mask, val_mask=val_mask, test_mask=test_mask).to(device)\n",
        "\n",
        "print(f\"Train size: {graph_data.train_mask.sum()}, Val size: {graph_data.val_mask.sum()}, Test size: {graph_data.test_mask.sum()}\")\n",
        "\n",
        "# Define the UHG Quadrance for prediction\n",
        "def uhg_quadrance(a, b):\n",
        "    \"\"\"Compute UHG quadrance between two points.\"\"\"\n",
        "    dot_product = torch.sum(a * b, dim=-1)\n",
        "    return 1 - (dot_product ** 2) / ((torch.sum(a ** 2, dim=-1) - a[:, -1] ** 2) * (torch.sum(b ** 2, dim=-1) - b[:, -1] ** 2))\n",
        "\n",
        "# Define the UHG GraphSAGE Layer\n",
        "class UHGGraphSAGELayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(UHGGraphSAGELayer, self).__init__()\n",
        "        self.weight_neigh = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.weight_self = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.weight_neigh)\n",
        "        nn.init.xavier_uniform_(self.weight_self)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        row, col = edge_index\n",
        "\n",
        "        # Neighbor aggregation\n",
        "        neigh_sum = torch.zeros_like(x)\n",
        "        neigh_sum.index_add_(0, row, x[col])\n",
        "        neigh_count = torch.zeros(x.size(0), device=x.device)\n",
        "        neigh_count.index_add_(0, row, torch.ones_like(row, dtype=torch.float))\n",
        "        neigh_count = torch.clamp(neigh_count.unsqueeze(1), min=1)\n",
        "        neigh_features = neigh_sum / neigh_count\n",
        "\n",
        "        # Apply linear transformations\n",
        "        neigh_transformed = torch.matmul(neigh_features, self.weight_neigh.t())\n",
        "        self_transformed = torch.matmul(x, self.weight_self.t())\n",
        "\n",
        "        # Combine using UHG-inspired operation (simplified addition)\n",
        "        combined = neigh_transformed + self_transformed\n",
        "\n",
        "        return F.relu(combined)\n",
        "\n",
        "# UHG GraphSAGE Model\n",
        "class UHGGraphSAGE(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout=0.2):\n",
        "        super(UHGGraphSAGE, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.layers.append(UHGGraphSAGELayer(in_channels, hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.layers.append(UHGGraphSAGELayer(hidden_channels, hidden_channels))\n",
        "        self.layers.append(UHGGraphSAGELayer(hidden_channels, out_channels))\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = self.dropout(F.relu(layer(x, edge_index)))\n",
        "        x = self.layers[-1](x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "in_channels = node_features_uhg.size(1)\n",
        "hidden_channels = 128\n",
        "out_channels = len(label_mapping)\n",
        "num_layers = 2\n",
        "\n",
        "# Define the loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Create a simple DataLoader\n",
        "batch_size = 16\n",
        "accumulation_steps = 4\n",
        "\n",
        "#Create Dataloader with smaller batch size\n",
        "train_loader = DataLoader(range(graph_data.train_mask.sum()), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Training process with gradient accumulation\n",
        "def train_with_accumulation(model, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    optimizer.zero_grad()  # Reset gradients\n",
        "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        # Get the features for the sampled nodes\n",
        "        x = graph_data.x[graph_data.train_mask][batch]\n",
        "        y = graph_data.y[graph_data.train_mask][batch]\n",
        "\n",
        "        # Create a subgraph for the batch\n",
        "        batch_node_ids = graph_data.train_mask.nonzero(as_tuple=True)[0][batch]\n",
        "        edge_mask = torch.isin(graph_data.edge_index[0], batch_node_ids) & torch.isin(graph_data.edge_index[1], batch_node_ids)\n",
        "        batch_edge_index = graph_data.edge_index[:, edge_mask]\n",
        "\n",
        "        # Relabel nodes to have consecutive indices\n",
        "        node_idx = torch.unique(batch_edge_index)\n",
        "        idx_map = {int(idx): i for i, idx in enumerate(node_idx)}\n",
        "        mapped_edge_index = torch.tensor([[idx_map[int(i)] for i in batch_edge_index[0]],\n",
        "                                          [idx_map[int(i)] for i in batch_edge_index[1]]],\n",
        "                                         dtype=torch.long,\n",
        "                                         device=device)\n",
        "\n",
        "        # Forward pass\n",
        "        out = model(x, mapped_edge_index)\n",
        "        loss = criterion(out, y) / accumulation_steps  # Scale loss by accumulation steps\n",
        "\n",
        "        loss.backward()  # Backpropagate the loss\n",
        "\n",
        "        # Accumulate gradients and update model weights every accumulation_steps batches\n",
        "        if (batch_idx + 1) % accumulation_steps == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()  # Update weights\n",
        "            optimizer.zero_grad()  # Reset gradients for next accumulation\n",
        "            total_loss += loss.item() * accumulation_steps  # Accumulate loss\n",
        "\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "# Evaluation function\n",
        "@torch.no_grad()\n",
        "def evaluate(model, mask):\n",
        "    model.eval()\n",
        "    node_indices = mask.nonzero(as_tuple=True)[0]\n",
        "    sub_x = graph_data.x[node_indices]\n",
        "    sub_y = graph_data.y[node_indices]\n",
        "    edge_mask = torch.isin(graph_data.edge_index[0], node_indices) & torch.isin(graph_data.edge_index[1], node_indices)\n",
        "    sub_edge_index = graph_data.edge_index[:, edge_mask]\n",
        "    node_idx = torch.unique(sub_edge_index)\n",
        "    idx_map = {int(idx): i for i, idx in enumerate(node_idx)}\n",
        "    mapped_edge_index = torch.tensor([[idx_map[int(i)] for i in sub_edge_index[0]],\n",
        "                                      [idx_map[int(i)] for i in sub_edge_index[1]]],\n",
        "                                     dtype=torch.long,\n",
        "                                     device=device)\n",
        "    out = model(sub_x, mapped_edge_index)\n",
        "\n",
        "    # For simplicity, using the model's output directly for classification\n",
        "    pred = out.argmax(dim=1)  # Choose the class with the highest logit\n",
        "    correct = (pred == sub_y).sum().item()\n",
        "    accuracy = correct / len(node_indices)  # Calculate accuracy\n",
        "\n",
        "    return accuracy  # Return the calculated accuracy\n",
        "\n",
        "\n",
        "# Set the learning rate\n",
        "best_lr = 0.01\n",
        "print(f\"Using learning rate: {best_lr}\")\n",
        "\n",
        "# Initialize the model\n",
        "model = UHGGraphSAGE(in_channels=in_channels, hidden_channels=hidden_channels,\n",
        "                     out_channels=out_channels, num_layers=num_layers).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=best_lr, weight_decay=1e-5)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 400\n",
        "best_val_acc = 0\n",
        "patience = 20\n",
        "counter = 0\n",
        "best_model_path = '/content/drive/MyDrive/best_uhg_graphsage_model.pth'\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    try:\n",
        "        # Train the model for one epoch\n",
        "        loss = train_with_accumulation(model, optimizer)\n",
        "\n",
        "\n",
        "        # Evaluate the model on validation and test sets\n",
        "        val_acc = evaluate(model, graph_data.val_mask)\n",
        "        test_acc = evaluate(model, graph_data.test_mask)\n",
        "\n",
        "        # Get current learning rate\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        # Adjust learning rate based on validation accuracy\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        # Check if current model is the best so far\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            counter = 0\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "        else:\n",
        "            counter += 1\n",
        "\n",
        "        # Print progress every 10 epochs\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch: {epoch}, Loss: {loss:.4f}, Val Accuracy: {val_acc:.4f}, Test Accuracy: {test_acc:.4f}, Learning Rate: {current_lr:.6f}')\n",
        "\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Error occurred in epoch {epoch}:\")\n",
        "        print(str(e))\n",
        "        break\n",
        "\n",
        "# After training, print the final learning rate\n",
        "final_lr = optimizer.param_groups[0]['lr']\n",
        "print(f\"Final Learning Rate: {final_lr:.6f}\")\n",
        "\n",
        "# Load the best model and evaluate on the test set\n",
        "if os.path.exists(best_model_path):\n",
        "    model.load_state_dict(torch.load(best_model_path))\n",
        "    final_test_acc = evaluate(model, graph_data.test_mask)\n",
        "    print(f\"Final Test Accuracy: {final_test_acc:.4f}\")\n",
        "else:\n",
        "    print(\"No best model found. Training might not have completed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G6KcGQlAjd2e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}