{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyOB5OGyJmnu47BibFFkevMG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bxqIWjWqXtiS","executionInfo":{"status":"ok","timestamp":1733444976553,"user_tz":300,"elapsed":10899,"user":{"displayName":"Zach Bovaird","userId":"05204517288903480331"}},"outputId":"65e83a20-0257-48fb-c72b-370b26e7708d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting uhg\n","  Downloading uhg-0.1.18-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from uhg) (2.5.1+cu121)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from uhg) (1.26.4)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from uhg) (1.13.1)\n","Collecting torch-geometric>=2.0.0 (from uhg)\n","  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->uhg) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->uhg) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->uhg) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->uhg) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->uhg) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->uhg) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->uhg) (1.3.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric>=2.0.0->uhg) (3.11.9)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric>=2.0.0->uhg) (5.9.5)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric>=2.0.0->uhg) (3.2.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric>=2.0.0->uhg) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric>=2.0.0->uhg) (4.66.6)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric>=2.0.0->uhg) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric>=2.0.0->uhg) (1.3.1)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric>=2.0.0->uhg) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric>=2.0.0->uhg) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric>=2.0.0->uhg) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric>=2.0.0->uhg) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric>=2.0.0->uhg) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric>=2.0.0->uhg) (1.18.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->uhg) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric>=2.0.0->uhg) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric>=2.0.0->uhg) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric>=2.0.0->uhg) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric>=2.0.0->uhg) (2024.8.30)\n","Downloading uhg-0.1.18-py3-none-any.whl (34 kB)\n","Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torch-geometric, uhg\n","Successfully installed torch-geometric-2.6.1 uhg-0.1.18\n"]}],"source":["!pip install uhg"]},{"cell_type":"code","source":["import uhg"],"metadata":{"id":"xSgtJe10XvXj","executionInfo":{"status":"ok","timestamp":1733444990048,"user_tz":300,"elapsed":5741,"user":{"displayName":"Zach Bovaird","userId":"05204517288903480331"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Intrusion Detection using Universal Hyperbolic Geometry (UHG).\n","This implementation leverages UHG's pure projective operations for better hierarchical learning.\n","Designed to run in Google Colab with GPU acceleration.\n","\"\"\"\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.neighbors import kneighbors_graph\n","from scipy.sparse import coo_matrix\n","from tqdm import tqdm\n","import uhg\n","from torch.utils.data import DataLoader\n","from torch_geometric.data import Data\n","import os\n","from typing import Tuple, Optional\n","from uhg.projective import ProjectiveUHG\n","\n","# Mount Google Drive\n","print(\"Mounting Google Drive...\")\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Device configuration - prioritize GPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","if torch.cuda.is_available():\n","    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n","else:\n","    print(\"Using CPU - Warning: Training may be slow\")\n","\n","# File paths\n","FILE_PATH = '/content/drive/MyDrive/CIC_data.csv'\n","MODEL_SAVE_PATH = '/content/drive/MyDrive/uhg_ids_model.pth'\n","RESULTS_PATH = '/content/drive/MyDrive/uhg_ids_results'\n","\n","# Create results directory\n","os.makedirs(RESULTS_PATH, exist_ok=True)\n","\n","def load_and_preprocess_data(file_path: str = FILE_PATH) -> Tuple[torch.Tensor, torch.Tensor, dict]:\n","    \"\"\"Load and preprocess the CIC dataset from Google Drive.\"\"\"\n","    print(f\"\\nLoading data from: {file_path}\")\n","\n","    # Load data\n","    data = pd.read_csv(file_path, low_memory=False)\n","    data.columns = data.columns.str.strip()\n","    data['Label'] = data['Label'].str.strip()\n","\n","    # Print initial statistics\n","    unique_labels = data['Label'].unique()\n","    print(f\"\\nUnique labels in the dataset: {unique_labels}\")\n","    label_counts = data['Label'].value_counts()\n","    print(\"\\nLabel distribution in the dataset:\")\n","    print(label_counts)\n","\n","    # Sample data (10%)\n","    data_sampled = data.sample(frac=0.10, random_state=42)\n","\n","    # Convert to numeric and handle missing values\n","    data_numeric = data_sampled.apply(pd.to_numeric, errors='coerce')\n","\n","    # Fill NaN values with column means\n","    data_filled = data_numeric.fillna(data_numeric.mean())\n","    data_filled = data_filled.replace([np.inf, -np.inf], np.nan)\n","    data_filled = data_filled.fillna(data_filled.max())\n","\n","    # Handle any remaining NaNs\n","    if data_filled.isnull().values.any():\n","        data_filled = data_filled.fillna(0)\n","\n","    # Extract labels and features\n","    labels = data_sampled['Label']\n","    features = data_filled.drop(columns=['Label'])\n","\n","    # Normalize features\n","    scaler = StandardScaler()\n","    features_scaled = scaler.fit_transform(features)\n","\n","    # Convert to tensors\n","    node_features = torch.tensor(features_scaled, dtype=torch.float32)\n","\n","    # Convert labels to numeric\n","    label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n","    labels_numeric = labels.map(label_mapping).values\n","    labels_tensor = torch.tensor(labels_numeric, dtype=torch.long)\n","\n","    print(\"\\nPreprocessing complete.\")\n","    print(f\"Feature shape: {node_features.shape}\")\n","    print(f\"Number of unique labels: {len(unique_labels)}\")\n","\n","    return node_features, labels_tensor, label_mapping\n","\n","def create_graph_data(node_features: torch.Tensor, labels: torch.Tensor, k: int = 2) -> Data:\n","    \"\"\"Create graph structure using UHG principles.\"\"\"\n","    print(\"\\nCreating graph structure...\")\n","\n","    # Convert features to numpy for sklearn\n","    features_np = node_features.cpu().numpy()\n","\n","    # Create k-nearest neighbors graph using sklearn (memory efficient)\n","    print(\"Computing KNN graph...\")\n","    knn_graph = kneighbors_graph(\n","        features_np,\n","        k,\n","        mode='connectivity',\n","        include_self=False\n","    )\n","\n","    # Convert to COO format\n","    knn_graph_coo = coo_matrix(knn_graph)\n","\n","    # Create edge index\n","    edge_index = torch.from_numpy(\n","        np.array([knn_graph_coo.row, knn_graph_coo.col])\n","    ).long().to(device)\n","\n","    print(f\"Edge index shape: {edge_index.shape}\")\n","\n","    # Add homogeneous coordinate to features\n","    node_features_uhg = torch.cat([\n","        node_features,\n","        torch.ones(node_features.size(0), 1, device=node_features.device)\n","    ], dim=1)\n","\n","    print(f\"Feature shape with homogeneous coordinate: {node_features_uhg.shape}\")\n","\n","    # Create train/val/test split\n","    total_samples = len(node_features_uhg)\n","    indices = torch.randperm(total_samples)\n","\n","    train_size = int(0.7 * total_samples)\n","    val_size = int(0.15 * total_samples)\n","\n","    train_mask = torch.zeros(total_samples, dtype=torch.bool)\n","    val_mask = torch.zeros(total_samples, dtype=torch.bool)\n","    test_mask = torch.zeros(total_samples, dtype=torch.bool)\n","\n","    train_mask[indices[:train_size]] = True\n","    val_mask[indices[train_size:train_size+val_size]] = True\n","    test_mask[indices[train_size+val_size:]] = True\n","\n","    print(f\"\\nTrain size: {train_mask.sum()}, Val size: {val_mask.sum()}, Test size: {test_mask.sum()}\")\n","\n","    return Data(\n","        x=node_features_uhg,\n","        edge_index=edge_index,\n","        y=labels,\n","        train_mask=train_mask,\n","        val_mask=val_mask,\n","        test_mask=test_mask\n","    ).to(device)\n","\n","# UHG Operations\n","def uhg_quadrance(a: torch.Tensor, b: torch.Tensor, eps: float = 1e-9) -> torch.Tensor:\n","    \"\"\"Compute UHG quadrance between two points.\"\"\"\n","    dot_product = torch.sum(a * b)  # For vectors\n","    return 1 - (dot_product ** 2) / (\n","        (torch.sum(a ** 2) - a[-1] ** 2 + eps) *\n","        (torch.sum(b ** 2) - b[-1] ** 2 + eps)\n","    )\n","\n","def uhg_spread(L: torch.Tensor, M: torch.Tensor, eps: float = 1e-9) -> torch.Tensor:\n","    \"\"\"Compute UHG spread between two lines.\"\"\"\n","    dot_product = torch.sum(L * M)  # For vectors\n","    return 1 - (dot_product ** 2) / (\n","        (torch.sum(L ** 2) - L[-1] ** 2 + eps) *\n","        (torch.sum(M ** 2) - M[-1] ** 2 + eps)\n","    )\n","\n","class UHGGraphSAGELayer(nn.Module):\n","    \"\"\"UHG-enhanced GraphSAGE layer using projective geometry.\"\"\"\n","    def __init__(self, in_features: int, out_features: int):\n","        super().__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","\n","        # Initialize weights for feature dimensions (excluding homogeneous coordinate)\n","        self.weight_neigh = nn.Parameter(torch.Tensor(out_features, in_features))\n","        self.weight_self = nn.Parameter(torch.Tensor(out_features, in_features))\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        nn.init.xavier_uniform_(self.weight_neigh)\n","        nn.init.xavier_uniform_(self.weight_self)\n","\n","    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n","        row, col = edge_index\n","\n","        # Split features and homogeneous coordinate\n","        features = x[:, :-1]  # All but last column\n","        homogeneous = x[:, -1:]  # Last column\n","\n","        # Neighbor aggregation using UHG distances\n","        neigh_sum = torch.zeros_like(features)\n","        neigh_weights = torch.zeros(features.size(0), device=features.device)\n","\n","        for i in range(len(row)):\n","            src, dst = row[i], col[i]\n","            # Compute UHG-based weight using full UHG coordinates\n","            weight = torch.exp(-uhg_quadrance(x[src], x[dst]))\n","            neigh_sum[src] += weight * features[dst]\n","            neigh_weights[src] += weight\n","\n","        # Normalize by weights\n","        neigh_weights = torch.clamp(neigh_weights.unsqueeze(1), min=1e-6)\n","        neigh_features = neigh_sum / neigh_weights\n","\n","        # Apply transformations to features only\n","        neigh_transformed = torch.matmul(neigh_features, self.weight_neigh.t())\n","        self_transformed = torch.matmul(features, self.weight_self.t())\n","\n","        # Combine features and add homogeneous coordinate back\n","        combined = neigh_transformed + self_transformed\n","        output = torch.cat([combined, homogeneous], dim=1)\n","\n","        return F.relu(output)\n","\n","class UHGGraphSAGE(nn.Module):\n","    \"\"\"UHG-enhanced GraphSAGE model for intrusion detection.\"\"\"\n","    def __init__(self, in_channels: int, hidden_channels: int, out_channels: int,\n","                 num_layers: int, dropout: float = 0.2):\n","        super().__init__()\n","        self.layers = nn.ModuleList()\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # Input features are one less than x.shape[1] due to homogeneous coordinate\n","        actual_in_channels = in_channels - 1\n","\n","        # Input layer\n","        self.layers.append(UHGGraphSAGELayer(actual_in_channels, hidden_channels))\n","\n","        # Hidden layers\n","        for _ in range(num_layers - 2):\n","            self.layers.append(UHGGraphSAGELayer(hidden_channels, hidden_channels))\n","\n","        # Output layer\n","        self.layers.append(UHGGraphSAGELayer(hidden_channels, out_channels))\n","\n","    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n","        # Process through layers\n","        for layer in self.layers[:-1]:\n","            x = layer(x, edge_index)\n","            x = self.dropout(x)\n","\n","        # Final layer\n","        x = self.layers[-1](x, edge_index)\n","\n","        # Use only feature part for classification, not homogeneous coordinate\n","        return x[:, :-1]\n","\n","def train_epoch(model: nn.Module, graph_data: Data, optimizer: torch.optim.Optimizer,\n","                criterion: nn.Module, batch_size: int = 16, accumulation_steps: int = 4) -> float:\n","    \"\"\"Train for one epoch using gradient accumulation.\"\"\"\n","    model.train()\n","    total_loss = 0\n","    train_loader = DataLoader(\n","        range(graph_data.train_mask.sum()),\n","        batch_size=batch_size,\n","        shuffle=True\n","    )\n","\n","    optimizer.zero_grad()\n","    for batch_idx, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n","        batch = batch.to(device)\n","\n","        # Get batch data\n","        x = graph_data.x[graph_data.train_mask][batch]\n","        y = graph_data.y[graph_data.train_mask][batch]\n","\n","        # Create subgraph\n","        batch_node_ids = graph_data.train_mask.nonzero(as_tuple=True)[0][batch]\n","        edge_mask = torch.isin(graph_data.edge_index[0], batch_node_ids) & \\\n","                   torch.isin(graph_data.edge_index[1], batch_node_ids)\n","        batch_edge_index = graph_data.edge_index[:, edge_mask]\n","\n","        # Relabel nodes\n","        node_idx = torch.unique(batch_edge_index)\n","        idx_map = {int(idx): i for i, idx in enumerate(node_idx)}\n","        mapped_edge_index = torch.tensor(\n","            [[idx_map[int(i)] for i in batch_edge_index[0]],\n","             [idx_map[int(i)] for i in batch_edge_index[1]]],\n","            dtype=torch.long,\n","            device=device\n","        )\n","\n","        # Forward pass\n","        out = model(x, mapped_edge_index)\n","        loss = criterion(out, y) / accumulation_steps\n","\n","        # Backward pass\n","        loss.backward()\n","\n","        # Update weights\n","        if (batch_idx + 1) % accumulation_steps == 0:\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            total_loss += loss.item() * accumulation_steps\n","\n","    return total_loss / len(train_loader)\n","\n","@torch.no_grad()\n","def evaluate(model: nn.Module, graph_data: Data, mask: torch.Tensor) -> float:\n","    \"\"\"Evaluate model on given mask.\"\"\"\n","    model.eval()\n","\n","    # Get masked data\n","    node_indices = mask.nonzero(as_tuple=True)[0]\n","    sub_x = graph_data.x[node_indices]\n","    sub_y = graph_data.y[node_indices]\n","\n","    # Create subgraph\n","    edge_mask = torch.isin(graph_data.edge_index[0], node_indices) & \\\n","                torch.isin(graph_data.edge_index[1], node_indices)\n","    sub_edge_index = graph_data.edge_index[:, edge_mask]\n","\n","    # Relabel nodes\n","    node_idx = torch.unique(sub_edge_index)\n","    idx_map = {int(idx): i for i, idx in enumerate(node_idx)}\n","    mapped_edge_index = torch.tensor(\n","        [[idx_map[int(i)] for i in sub_edge_index[0]],\n","         [idx_map[int(i)] for i in sub_edge_index[1]]],\n","        dtype=torch.long,\n","        device=device\n","    )\n","\n","    # Forward pass\n","    out = model(sub_x, mapped_edge_index)\n","    pred = out.argmax(dim=1)\n","\n","    # Calculate accuracy\n","    correct = (pred == sub_y).sum().item()\n","    accuracy = correct / len(node_indices)\n","\n","    return accuracy\n","\n","def main():\n","    \"\"\"Main training function.\"\"\"\n","    # Load and preprocess data\n","    node_features, labels, label_mapping = load_and_preprocess_data()\n","    graph_data = create_graph_data(node_features, labels)\n","\n","    # Model parameters\n","    in_channels = graph_data.x.size(1)\n","    hidden_channels = 128\n","    out_channels = len(label_mapping)\n","    num_layers = 2\n","\n","    # Initialize model and optimizer\n","    model = UHGGraphSAGE(\n","        in_channels=in_channels,\n","        hidden_channels=hidden_channels,\n","        out_channels=out_channels,\n","        num_layers=num_layers\n","    ).to(device)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","        optimizer, mode='max', factor=0.5, patience=10\n","    )\n","    criterion = nn.CrossEntropyLoss()\n","\n","    # Training loop\n","    num_epochs = 400\n","    best_val_acc = 0\n","    patience = 20\n","    counter = 0\n","\n","    print(\"\\nStarting training...\")\n","    for epoch in range(1, num_epochs + 1):\n","        try:\n","            # Train\n","            loss = train_epoch(model, graph_data, optimizer, criterion)\n","\n","            # Evaluate\n","            val_acc = evaluate(model, graph_data, graph_data.val_mask)\n","            test_acc = evaluate(model, graph_data, graph_data.test_mask)\n","\n","            # Update learning rate\n","            scheduler.step(val_acc)\n","\n","            # Save best model\n","            if val_acc > best_val_acc:\n","                best_val_acc = val_acc\n","                counter = 0\n","                torch.save(model.state_dict(), MODEL_SAVE_PATH)\n","                print(f\"\\nNew best model saved! Validation accuracy: {val_acc:.4f}\")\n","            else:\n","                counter += 1\n","\n","            # Print progress\n","            if epoch % 10 == 0:\n","                print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Accuracy: {val_acc:.4f}, '\n","                      f'Test Accuracy: {test_acc:.4f}')\n","\n","            # Early stopping\n","            if counter >= patience:\n","                print(\"Early stopping triggered!\")\n","                break\n","\n","        except RuntimeError as e:\n","            print(f\"\\nError in epoch {epoch}: {str(e)}\")\n","            break\n","\n","    # Final evaluation\n","    if os.path.exists(MODEL_SAVE_PATH):\n","        print(\"\\nLoading best model for final evaluation...\")\n","        model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n","        final_test_acc = evaluate(model, graph_data, graph_data.test_mask)\n","        print(f\"\\nFinal Test Accuracy: {final_test_acc:.4f}\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2iM7AZRwX1sU","executionInfo":{"status":"ok","timestamp":1733451673862,"user_tz":300,"elapsed":4490846,"user":{"displayName":"Zach Bovaird","userId":"05204517288903480331"}},"outputId":"014a9f1e-b1b2-4074-988d-fa42b4b794fe"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounting Google Drive...\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Using GPU: Tesla T4\n","\n","Loading data from: /content/drive/MyDrive/CIC_data.csv\n","\n","Unique labels in the dataset: ['BENIGN' 'DDoS' 'PortScan' 'Bot' 'Infiltration'\n"," 'Web Attack � Brute Force' 'Web Attack � XSS'\n"," 'Web Attack � Sql Injection' 'FTP-Patator' 'SSH-Patator' 'DoS slowloris'\n"," 'DoS Slowhttptest' 'DoS Hulk' 'DoS GoldenEye' 'Heartbleed']\n","\n","Label distribution in the dataset:\n","Label\n","BENIGN                        2273097\n","DoS Hulk                       231073\n","PortScan                       158930\n","DDoS                           128027\n","DoS GoldenEye                   10293\n","FTP-Patator                      7938\n","SSH-Patator                      5897\n","DoS slowloris                    5796\n","DoS Slowhttptest                 5499\n","Bot                              1966\n","Web Attack � Brute Force         1507\n","Web Attack � XSS                  652\n","Infiltration                       36\n","Web Attack � Sql Injection         21\n","Heartbleed                         11\n","Name: count, dtype: int64\n","\n","Preprocessing complete.\n","Feature shape: torch.Size([283074, 77])\n","Number of unique labels: 15\n","\n","Creating graph structure...\n","Computing KNN graph...\n","Edge index shape: torch.Size([2, 566148])\n","Feature shape with homogeneous coordinate: torch.Size([283074, 78])\n","\n","Train size: 198151, Val size: 42461, Test size: 42462\n","\n","Starting training...\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 12385/12385 [00:47<00:00, 259.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","New best model saved! Validation accuracy: 0.9583\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 12385/12385 [00:47<00:00, 259.25it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 256.01it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 256.52it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 256.39it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 256.86it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 257.05it/s]\n","Training: 100%|██████████| 12385/12385 [00:47<00:00, 258.08it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 257.62it/s]\n","Training: 100%|██████████| 12385/12385 [00:47<00:00, 258.42it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 010, Loss: 0.0344, Val Accuracy: 0.9382, Test Accuracy: 0.9360\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 12385/12385 [00:48<00:00, 257.73it/s]\n","Training: 100%|██████████| 12385/12385 [00:47<00:00, 258.19it/s]\n","Training: 100%|██████████| 12385/12385 [00:47<00:00, 258.39it/s]\n","Training: 100%|██████████| 12385/12385 [00:47<00:00, 258.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","New best model saved! Validation accuracy: 0.9622\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 12385/12385 [00:48<00:00, 257.70it/s]\n","Training: 100%|██████████| 12385/12385 [00:47<00:00, 258.68it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 253.35it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 254.44it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 254.32it/s]\n","Training: 100%|██████████| 12385/12385 [00:49<00:00, 252.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 020, Loss: 0.0230, Val Accuracy: 0.9595, Test Accuracy: 0.9584\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 12385/12385 [00:48<00:00, 254.79it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 255.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","New best model saved! Validation accuracy: 0.9662\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 12385/12385 [00:48<00:00, 255.11it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 255.43it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 254.15it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 255.81it/s]\n","Training: 100%|██████████| 12385/12385 [00:49<00:00, 252.29it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 256.36it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 256.64it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 255.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 030, Loss: 0.0211, Val Accuracy: 0.9532, Test Accuracy: 0.9505\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 12385/12385 [00:48<00:00, 255.77it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 253.16it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 257.16it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","New best model saved! Validation accuracy: 0.9673\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 12385/12385 [00:48<00:00, 256.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","New best model saved! Validation accuracy: 0.9692\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 12385/12385 [00:48<00:00, 256.87it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 256.63it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 253.97it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 254.06it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 255.59it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 255.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 040, Loss: 0.0206, Val Accuracy: 0.9647, Test Accuracy: 0.9630\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 12385/12385 [00:48<00:00, 255.19it/s]\n","Training: 100%|██████████| 12385/12385 [00:49<00:00, 251.27it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 253.95it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 256.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","New best model saved! Validation accuracy: 0.9697\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 12385/12385 [00:48<00:00, 256.58it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 256.08it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 256.50it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 255.91it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 257.04it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 255.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 050, Loss: 0.0211, Val Accuracy: 0.9616, Test Accuracy: 0.9604\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 12385/12385 [00:48<00:00, 255.50it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 256.73it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 257.22it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 256.34it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 256.64it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 257.38it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 255.68it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 256.23it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 257.00it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 256.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 060, Loss: 0.0179, Val Accuracy: 0.9583, Test Accuracy: 0.9556\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 12385/12385 [00:48<00:00, 254.14it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 254.62it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 257.31it/s]\n","Training: 100%|██████████| 12385/12385 [00:48<00:00, 255.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Early stopping triggered!\n","\n","Loading best model for final evaluation...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-8-cece2322bfb3>:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n"]},{"output_type":"stream","name":"stdout","text":["\n","Final Test Accuracy: 0.9696\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"KQMdhdfsZMQR"},"execution_count":null,"outputs":[]}]}